{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library design for CTP-03, Chr21 9by36 test\n",
    "\n",
    "by Pu Zheng\n",
    "\n",
    "This library design is for human chr21\n",
    "\n",
    "Test lighting-up-8 spots strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#minimum imports:\n",
    "import time,os,sys,glob\n",
    "import numpy as np\n",
    "import khmer\n",
    "sys.path.append(r'/n/home13/pzheng/Documents/python-functions/python-functions-library')\n",
    "\n",
    "from LibraryConstruction import fastaread,fastawrite,fastacombine\n",
    "import LibraryDesigner as ld\n",
    "import LibraryConstruction as lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Assign region into TADs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Match_TADs(master_folder, TAD_ref, report_folder,\n",
    "              verbose=True, save=True):\n",
    "    '''Function to match regions with a TAD reference\n",
    "    Input: \n",
    "    master_folder: master directory for the whole dataset, string\n",
    "    TAD_ref: filename for TAD reference, string\n",
    "    report_folder: directory for probe reports, string'''\n",
    "    import os, glob, sys\n",
    "    import LibraryDesigner as ld\n",
    "    import numpy as np\n",
    "    import cPickle as pickle\n",
    "    \n",
    "    def Read_TAD_ref(master_folder=master_folder, TAD_ref=TAD_ref):\n",
    "        _tad_dics = [];\n",
    "        with open(master_folder+os.sep+TAD_ref) as _ref_handle:\n",
    "            _lines = _ref_handle.readlines();\n",
    "            for _line in _lines:\n",
    "                _chrom = _line.split(':')[0]\n",
    "                _reg_str = _line.split(':')[1].split('\\n')[0];\n",
    "                _start,_stop = _reg_str.split('-');\n",
    "                _tad_dic = {'chr':_chrom, 'start':int(_start), 'stop':int(_stop)}\n",
    "                _tad_dics.append(_tad_dic);\n",
    "        return sorted(_tad_dics, key=lambda d:d['start']);\n",
    "    \n",
    "    def Region_to_TAD(tad_dics, report_filename):\n",
    "        _pb = ld.pb_reports_class()\n",
    "        _pb.load_pbr(report_filename)\n",
    "        # get its region status\n",
    "        _reg_id = int(_pb.pb_reports_keep.values()[0]['reg_name'].split('reg')[1].split('_')[1])\n",
    "        _chrom = _pb.pb_reports_keep.values()[0]['reg_name'].split(':')[0]\n",
    "        _start, _stop = _pb.pb_reports_keep.values()[0]['reg_name'].split(':')[1].split('_')[0].split('-')\n",
    "        _start = int(_start);\n",
    "        _stop = int(_stop);\n",
    "        if _start > _stop:\n",
    "            _start, _stop = _stop, _start\n",
    "        _reg_len = abs(_stop - _start)\n",
    "        # initialize tad identity of this region\n",
    "        _tad_id = -1;\n",
    "        for i in range(len(tad_dics)):\n",
    "            _dic = tad_dics[i];\n",
    "            if _chrom == _dic['chr']:\n",
    "                _overlap = min(_stop, _dic['stop']) - max(_start, _dic['start']);\n",
    "                if _overlap > _reg_len / 2:\n",
    "                    _tad_id = i; # assign tad id\n",
    "                    break\n",
    "    \n",
    "        return _reg_id, _tad_id, len(_pb.pb_reports_keep)\n",
    "    \n",
    "    def Extra_Region_Assigning(tad_id_dic):\n",
    "        '''Try to assign region to TADs as much as possible\n",
    "        '''\n",
    "        # calculate how many region has been assigned to each TAD\n",
    "        _v,_c = np.unique(tad_id_dic.values(),return_counts=True)\n",
    "        _reg_num_dic = dict(zip(_v,_c)) # dictionary for region number of each TAD\n",
    "\n",
    "        # maximum gap size to be filled\n",
    "        _gap_max = 4 \n",
    "\n",
    "        # new_id_dic\n",
    "        _new_id_dic = tad_id_dic.copy();\n",
    "\n",
    "        # Starting filling gaps!\n",
    "        _gap = 0;\n",
    "        _prev_value = -1;\n",
    "        for _key, _value in sorted(_new_id_dic.items()):\n",
    "            # start a gap \n",
    "            if _gap == 0 and _value == -1: \n",
    "                _prev_tad = _prev_value\n",
    "                _gap = 1; # turn on gap\n",
    "                _key_ingap = [_key] # start recording keys in gap\n",
    "\n",
    "            # continue a gap\n",
    "            elif _gap == 1 and _value == -1:\n",
    "                _key_ingap.append(_key)\n",
    "\n",
    "            # stop a gap!\n",
    "            elif _gap == 1 and _value > -1:\n",
    "                _gap = 0; # stop counting gap\n",
    "                _next_tad = _value \n",
    "                # if the gap is not huge, try to make up\n",
    "                if len(_key_ingap) <= _gap_max: \n",
    "                    if _prev_tad == -1: # don't fill any gap at beginning\n",
    "                        continue \n",
    "                    elif len(_key_ingap)/2*2 == len(_key_ingap): # gap size is even number\n",
    "                        for i in range(len(_key_ingap)/2):\n",
    "                            _new_id_dic[_key_ingap[i]] = _prev_tad\n",
    "                            _new_id_dic[_key_ingap[i+len(_key_ingap)/2]] = _next_tad\n",
    "                    else: # gap size is odd number\n",
    "                        for i in range(len(_key_ingap)/2):\n",
    "                            _new_id_dic[_key_ingap[i]] = _prev_tad\n",
    "                            _new_id_dic[_key_ingap[i+len(_key_ingap)/2+1]] = _next_tad\n",
    "                        if _reg_num_dic[_prev_tad] <= _reg_num_dic[_next_tad]:\n",
    "                            _new_id_dic[_key_ingap[len(_key_ingap)/2]] = _prev_tad\n",
    "                        else:\n",
    "                            _new_id_dic[_key_ingap[len(_key_ingap)/2]] = _next_tad\n",
    "\n",
    "            _prev_value = _value # store previous tad info\n",
    "\n",
    "        return _new_id_dic   \n",
    "    \n",
    "    def Save_dics(master_folder, tad_dics, reg_len_dic, new_id_dic):\n",
    "        # save tad dics\n",
    "        tad_dic_file = open(master_folder+os.sep+'TAD_dic_list.pkl','w');\n",
    "        pickle.dump(tad_dics, tad_dic_file);\n",
    "        tad_dic_file.close()\n",
    "        # save region length dic\n",
    "        reg_len_dic_file = open(master_folder+os.sep+'region_length.pkl','w');\n",
    "        pickle.dump(reg_len_dic, reg_len_dic_file);\n",
    "        reg_len_dic_file.close()        \n",
    "        # save region_to_tad dic\n",
    "        reg_to_tad_file = open(master_folder+os.sep+'region_to_TAD.pkl','w');\n",
    "        pickle.dump(new_id_dic, reg_to_tad_file);\n",
    "        reg_to_tad_file.close() \n",
    "\n",
    "    if verbose:\n",
    "        print '- Start reading TAD reference', TAD_ref\n",
    "    tad_dics = Read_TAD_ref()\n",
    "    \n",
    "    if verbose:\n",
    "        print '- Start reading probe reports'\n",
    "\n",
    "    files = glob.glob(report_folder+os.sep+r'*.pbr')\n",
    "    tad_id_dic = {} # store assigned tad id\n",
    "    reg_len_dic = {} # store number of probes in each region\n",
    "    \n",
    "    for _file in sorted(files):\n",
    "        reg_id, tad_id, reg_len = Region_to_TAD(tad_dics, _file)\n",
    "        tad_id_dic[reg_id] = tad_id; # update tad id dic\n",
    "        reg_len_dic[reg_id] = reg_len; # update region length dic\n",
    "        if verbose:\n",
    "            print '--', os.path.basename(_file), 'tad_id:', tad_id, 'size:', reg_len\n",
    "\n",
    "            \n",
    "    new_id_dic = Extra_Region_Assigning(tad_id_dic)\n",
    "    \n",
    "    if save:\n",
    "        Save_dics(master_folder=master_folder,\n",
    "                 tad_dics=tad_dics,\n",
    "                 reg_len_dic=reg_len_dic,\n",
    "                 new_id_dic=new_id_dic);\n",
    "\n",
    "    \n",
    "    return tad_dics, tad_id_dic, reg_len_dic, new_id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Start reading TAD reference chr21_TADs_splitted.bed\n",
      "- Start reading probe reports\n",
      "-- reg_101.pbr tad_id: -1 size: 343\n",
      "-- reg_102.pbr tad_id: -1 size: 399\n",
      "-- reg_103.pbr tad_id: -1 size: 400\n",
      "-- reg_121.pbr tad_id: -1 size: 370\n",
      "-- reg_122.pbr tad_id: -1 size: 400\n",
      "-- reg_123.pbr tad_id: -1 size: 400\n",
      "-- reg_130.pbr tad_id: -1 size: 400\n",
      "-- reg_131.pbr tad_id: -1 size: 257\n",
      "-- reg_132.pbr tad_id: -1 size: 278\n",
      "-- reg_156.pbr tad_id: -1 size: 363\n",
      "-- reg_157.pbr tad_id: -1 size: 281\n",
      "-- reg_195.pbr tad_id: -1 size: 216\n",
      "-- reg_196.pbr tad_id: -1 size: 386\n",
      "-- reg_209.pbr tad_id: 0 size: 215\n",
      "-- reg_211.pbr tad_id: 0 size: 239\n",
      "-- reg_213.pbr tad_id: 0 size: 216\n",
      "-- reg_266.pbr tad_id: 1 size: 221\n",
      "-- reg_281.pbr tad_id: 1 size: 400\n",
      "-- reg_282.pbr tad_id: 1 size: 367\n",
      "-- reg_283.pbr tad_id: 1 size: 400\n",
      "-- reg_284.pbr tad_id: 1 size: 400\n",
      "-- reg_285.pbr tad_id: 1 size: 400\n",
      "-- reg_286.pbr tad_id: 1 size: 400\n",
      "-- reg_287.pbr tad_id: 1 size: 400\n",
      "-- reg_288.pbr tad_id: 1 size: 400\n",
      "-- reg_289.pbr tad_id: 1 size: 334\n",
      "-- reg_290.pbr tad_id: 1 size: 400\n",
      "-- reg_291.pbr tad_id: 1 size: 400\n",
      "-- reg_292.pbr tad_id: 1 size: 400\n",
      "-- reg_293.pbr tad_id: 1 size: 400\n",
      "-- reg_294.pbr tad_id: 1 size: 400\n",
      "-- reg_295.pbr tad_id: 1 size: 400\n",
      "-- reg_296.pbr tad_id: 1 size: 400\n",
      "-- reg_297.pbr tad_id: 1 size: 400\n",
      "-- reg_298.pbr tad_id: 1 size: 400\n",
      "-- reg_299.pbr tad_id: 1 size: 400\n",
      "-- reg_300.pbr tad_id: 1 size: 400\n",
      "-- reg_301.pbr tad_id: 1 size: 400\n",
      "-- reg_302.pbr tad_id: 1 size: 400\n",
      "-- reg_303.pbr tad_id: 1 size: 400\n",
      "-- reg_304.pbr tad_id: 1 size: 400\n",
      "-- reg_305.pbr tad_id: 1 size: 400\n",
      "-- reg_306.pbr tad_id: 1 size: 400\n",
      "-- reg_307.pbr tad_id: 1 size: 400\n",
      "-- reg_308.pbr tad_id: 1 size: 400\n",
      "-- reg_309.pbr tad_id: 1 size: 400\n",
      "-- reg_310.pbr tad_id: 1 size: 400\n",
      "-- reg_311.pbr tad_id: 1 size: 400\n",
      "-- reg_312.pbr tad_id: 1 size: 400\n",
      "-- reg_313.pbr tad_id: 1 size: 400\n",
      "-- reg_314.pbr tad_id: 1 size: 400\n",
      "-- reg_315.pbr tad_id: 1 size: 400\n",
      "-- reg_316.pbr tad_id: 1 size: 400\n",
      "-- reg_317.pbr tad_id: 1 size: 400\n",
      "-- reg_318.pbr tad_id: 2 size: 400\n",
      "-- reg_319.pbr tad_id: 2 size: 395\n",
      "-- reg_320.pbr tad_id: 2 size: 400\n",
      "-- reg_321.pbr tad_id: 2 size: 400\n",
      "-- reg_322.pbr tad_id: 2 size: 400\n",
      "-- reg_323.pbr tad_id: 2 size: 400\n",
      "-- reg_324.pbr tad_id: 2 size: 400\n",
      "-- reg_325.pbr tad_id: 2 size: 400\n",
      "-- reg_326.pbr tad_id: 2 size: 400\n",
      "-- reg_327.pbr tad_id: 2 size: 400\n",
      "-- reg_328.pbr tad_id: 2 size: 400\n",
      "-- reg_329.pbr tad_id: 2 size: 400\n",
      "-- reg_330.pbr tad_id: 2 size: 400\n",
      "-- reg_331.pbr tad_id: 2 size: 400\n",
      "-- reg_332.pbr tad_id: 2 size: 400\n",
      "-- reg_333.pbr tad_id: 2 size: 400\n",
      "-- reg_334.pbr tad_id: 2 size: 400\n",
      "-- reg_335.pbr tad_id: 2 size: 400\n",
      "-- reg_336.pbr tad_id: 2 size: 400\n",
      "-- reg_337.pbr tad_id: 2 size: 400\n",
      "-- reg_338.pbr tad_id: 2 size: 400\n",
      "-- reg_339.pbr tad_id: 2 size: 400\n",
      "-- reg_340.pbr tad_id: 2 size: 400\n",
      "-- reg_341.pbr tad_id: 2 size: 400\n",
      "-- reg_342.pbr tad_id: 2 size: 400\n",
      "-- reg_343.pbr tad_id: 2 size: 400\n",
      "-- reg_348.pbr tad_id: 2 size: 400\n",
      "-- reg_349.pbr tad_id: 2 size: 400\n",
      "-- reg_350.pbr tad_id: 2 size: 400\n",
      "-- reg_351.pbr tad_id: 2 size: 400\n",
      "-- reg_352.pbr tad_id: 2 size: 400\n",
      "-- reg_353.pbr tad_id: 2 size: 283\n",
      "-- reg_354.pbr tad_id: 2 size: 202\n",
      "-- reg_355.pbr tad_id: 2 size: 205\n",
      "-- reg_356.pbr tad_id: 2 size: 400\n",
      "-- reg_357.pbr tad_id: 2 size: 400\n",
      "-- reg_358.pbr tad_id: 2 size: 400\n",
      "-- reg_359.pbr tad_id: 2 size: 400\n",
      "-- reg_360.pbr tad_id: 2 size: 400\n",
      "-- reg_361.pbr tad_id: -1 size: 400\n",
      "-- reg_362.pbr tad_id: 3 size: 400\n",
      "-- reg_363.pbr tad_id: 3 size: 400\n",
      "-- reg_364.pbr tad_id: 3 size: 400\n",
      "-- reg_365.pbr tad_id: 3 size: 400\n",
      "-- reg_366.pbr tad_id: 3 size: 400\n",
      "-- reg_367.pbr tad_id: 3 size: 400\n",
      "-- reg_368.pbr tad_id: 3 size: 400\n",
      "-- reg_369.pbr tad_id: 3 size: 400\n",
      "-- reg_370.pbr tad_id: 3 size: 400\n",
      "-- reg_371.pbr tad_id: 3 size: 400\n",
      "-- reg_372.pbr tad_id: 3 size: 400\n",
      "-- reg_373.pbr tad_id: 3 size: 400\n",
      "-- reg_374.pbr tad_id: 3 size: 400\n",
      "-- reg_375.pbr tad_id: 3 size: 400\n",
      "-- reg_376.pbr tad_id: 3 size: 400\n",
      "-- reg_377.pbr tad_id: 3 size: 400\n",
      "-- reg_378.pbr tad_id: 3 size: 400\n",
      "-- reg_379.pbr tad_id: 3 size: 400\n",
      "-- reg_380.pbr tad_id: 3 size: 400\n",
      "-- reg_381.pbr tad_id: 3 size: 400\n",
      "-- reg_382.pbr tad_id: 3 size: 400\n",
      "-- reg_383.pbr tad_id: 3 size: 400\n",
      "-- reg_384.pbr tad_id: 3 size: 400\n",
      "-- reg_385.pbr tad_id: 3 size: 400\n",
      "-- reg_386.pbr tad_id: 3 size: 400\n",
      "-- reg_387.pbr tad_id: 3 size: 400\n",
      "-- reg_388.pbr tad_id: 3 size: 400\n",
      "-- reg_389.pbr tad_id: 4 size: 400\n",
      "-- reg_390.pbr tad_id: 4 size: 400\n",
      "-- reg_391.pbr tad_id: 4 size: 400\n",
      "-- reg_392.pbr tad_id: 4 size: 400\n",
      "-- reg_393.pbr tad_id: 4 size: 400\n",
      "-- reg_394.pbr tad_id: 4 size: 400\n",
      "-- reg_395.pbr tad_id: 4 size: 400\n",
      "-- reg_396.pbr tad_id: 4 size: 400\n",
      "-- reg_397.pbr tad_id: 4 size: 400\n",
      "-- reg_398.pbr tad_id: 4 size: 400\n",
      "-- reg_399.pbr tad_id: 4 size: 400\n",
      "-- reg_400.pbr tad_id: 4 size: 400\n",
      "-- reg_401.pbr tad_id: 4 size: 400\n",
      "-- reg_402.pbr tad_id: 4 size: 400\n",
      "-- reg_403.pbr tad_id: 4 size: 400\n",
      "-- reg_404.pbr tad_id: 4 size: 400\n",
      "-- reg_405.pbr tad_id: 4 size: 400\n",
      "-- reg_406.pbr tad_id: 4 size: 400\n",
      "-- reg_407.pbr tad_id: 4 size: 400\n",
      "-- reg_408.pbr tad_id: 4 size: 400\n",
      "-- reg_409.pbr tad_id: 4 size: 400\n",
      "-- reg_410.pbr tad_id: 4 size: 400\n",
      "-- reg_411.pbr tad_id: 4 size: 400\n",
      "-- reg_412.pbr tad_id: 4 size: 400\n",
      "-- reg_413.pbr tad_id: 4 size: 400\n",
      "-- reg_414.pbr tad_id: 4 size: 400\n",
      "-- reg_415.pbr tad_id: 4 size: 400\n",
      "-- reg_416.pbr tad_id: 4 size: 400\n",
      "-- reg_417.pbr tad_id: -1 size: 400\n",
      "-- reg_418.pbr tad_id: -1 size: 400\n",
      "-- reg_419.pbr tad_id: -1 size: 400\n",
      "-- reg_420.pbr tad_id: 5 size: 400\n",
      "-- reg_421.pbr tad_id: 5 size: 400\n",
      "-- reg_422.pbr tad_id: 5 size: 400\n",
      "-- reg_423.pbr tad_id: 5 size: 400\n",
      "-- reg_424.pbr tad_id: 5 size: 400\n",
      "-- reg_425.pbr tad_id: 5 size: 400\n",
      "-- reg_426.pbr tad_id: 5 size: 400\n",
      "-- reg_427.pbr tad_id: 5 size: 400\n",
      "-- reg_428.pbr tad_id: 5 size: 400\n",
      "-- reg_429.pbr tad_id: 5 size: 400\n",
      "-- reg_430.pbr tad_id: 5 size: 400\n",
      "-- reg_431.pbr tad_id: 5 size: 400\n",
      "-- reg_432.pbr tad_id: 5 size: 400\n",
      "-- reg_433.pbr tad_id: 5 size: 400\n",
      "-- reg_434.pbr tad_id: 5 size: 400\n",
      "-- reg_435.pbr tad_id: 5 size: 400\n",
      "-- reg_436.pbr tad_id: 5 size: 400\n",
      "-- reg_437.pbr tad_id: 5 size: 400\n",
      "-- reg_438.pbr tad_id: 5 size: 400\n",
      "-- reg_439.pbr tad_id: 5 size: 400\n",
      "-- reg_440.pbr tad_id: 5 size: 400\n",
      "-- reg_441.pbr tad_id: 5 size: 400\n",
      "-- reg_442.pbr tad_id: 5 size: 400\n",
      "-- reg_443.pbr tad_id: 5 size: 268\n",
      "-- reg_445.pbr tad_id: 5 size: 222\n",
      "-- reg_446.pbr tad_id: 5 size: 240\n",
      "-- reg_447.pbr tad_id: 5 size: 400\n",
      "-- reg_448.pbr tad_id: 5 size: 400\n",
      "-- reg_449.pbr tad_id: 5 size: 400\n",
      "-- reg_450.pbr tad_id: 5 size: 400\n",
      "-- reg_451.pbr tad_id: 6 size: 400\n",
      "-- reg_452.pbr tad_id: 6 size: 400\n",
      "-- reg_453.pbr tad_id: 6 size: 400\n",
      "-- reg_454.pbr tad_id: 6 size: 400\n",
      "-- reg_455.pbr tad_id: 6 size: 400\n",
      "-- reg_456.pbr tad_id: 6 size: 400\n",
      "-- reg_457.pbr tad_id: 6 size: 400\n",
      "-- reg_458.pbr tad_id: 6 size: 400\n",
      "-- reg_459.pbr tad_id: 6 size: 400\n",
      "-- reg_460.pbr tad_id: 6 size: 400\n",
      "-- reg_461.pbr tad_id: 6 size: 400\n",
      "-- reg_462.pbr tad_id: 6 size: 400\n",
      "-- reg_463.pbr tad_id: 6 size: 400\n",
      "-- reg_464.pbr tad_id: 6 size: 400\n",
      "-- reg_465.pbr tad_id: 6 size: 400\n",
      "-- reg_466.pbr tad_id: 6 size: 400\n",
      "-- reg_467.pbr tad_id: 6 size: 400\n",
      "-- reg_468.pbr tad_id: 6 size: 400\n",
      "-- reg_469.pbr tad_id: 6 size: 400\n",
      "-- reg_470.pbr tad_id: 6 size: 400\n",
      "-- reg_471.pbr tad_id: 6 size: 400\n",
      "-- reg_472.pbr tad_id: 6 size: 400\n",
      "-- reg_473.pbr tad_id: 6 size: 400\n",
      "-- reg_474.pbr tad_id: 6 size: 400\n",
      "-- reg_475.pbr tad_id: 6 size: 400\n",
      "-- reg_476.pbr tad_id: 6 size: 400\n",
      "-- reg_477.pbr tad_id: 6 size: 400\n",
      "-- reg_478.pbr tad_id: 6 size: 400\n",
      "-- reg_479.pbr tad_id: 6 size: 400\n",
      "-- reg_480.pbr tad_id: 6 size: 400\n",
      "-- reg_481.pbr tad_id: 6 size: 400\n",
      "-- reg_482.pbr tad_id: 7 size: 400\n",
      "-- reg_483.pbr tad_id: 7 size: 400\n",
      "-- reg_484.pbr tad_id: 7 size: 400\n",
      "-- reg_485.pbr tad_id: 7 size: 400\n",
      "-- reg_486.pbr tad_id: 7 size: 400\n",
      "-- reg_487.pbr tad_id: 7 size: 400\n",
      "-- reg_488.pbr tad_id: 7 size: 400\n",
      "-- reg_489.pbr tad_id: 7 size: 400\n",
      "-- reg_490.pbr tad_id: 7 size: 400\n",
      "-- reg_491.pbr tad_id: 7 size: 400\n",
      "-- reg_492.pbr tad_id: 7 size: 400\n",
      "-- reg_493.pbr tad_id: 7 size: 400\n",
      "-- reg_494.pbr tad_id: 7 size: 400\n",
      "-- reg_495.pbr tad_id: 7 size: 400\n",
      "-- reg_496.pbr tad_id: 7 size: 400\n",
      "-- reg_497.pbr tad_id: 7 size: 400\n",
      "-- reg_498.pbr tad_id: 7 size: 400\n",
      "-- reg_499.pbr tad_id: 7 size: 400\n",
      "-- reg_500.pbr tad_id: 7 size: 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- reg_501.pbr tad_id: 7 size: 400\n",
      "-- reg_502.pbr tad_id: 7 size: 400\n",
      "-- reg_503.pbr tad_id: 7 size: 400\n",
      "-- reg_504.pbr tad_id: 7 size: 400\n",
      "-- reg_505.pbr tad_id: 7 size: 400\n",
      "-- reg_506.pbr tad_id: 7 size: 400\n",
      "-- reg_507.pbr tad_id: 7 size: 400\n",
      "-- reg_508.pbr tad_id: 7 size: 400\n",
      "-- reg_509.pbr tad_id: 7 size: 400\n",
      "-- reg_510.pbr tad_id: 7 size: 400\n",
      "-- reg_511.pbr tad_id: 7 size: 400\n",
      "-- reg_512.pbr tad_id: 7 size: 400\n",
      "-- reg_513.pbr tad_id: 7 size: 400\n",
      "-- reg_514.pbr tad_id: 8 size: 400\n",
      "-- reg_515.pbr tad_id: 8 size: 400\n",
      "-- reg_516.pbr tad_id: 8 size: 400\n",
      "-- reg_517.pbr tad_id: 8 size: 400\n",
      "-- reg_518.pbr tad_id: 8 size: 400\n",
      "-- reg_519.pbr tad_id: 8 size: 400\n",
      "-- reg_520.pbr tad_id: 8 size: 400\n",
      "-- reg_521.pbr tad_id: 8 size: 400\n",
      "-- reg_522.pbr tad_id: 8 size: 400\n",
      "-- reg_523.pbr tad_id: 8 size: 400\n",
      "-- reg_524.pbr tad_id: 8 size: 400\n",
      "-- reg_525.pbr tad_id: 8 size: 400\n",
      "-- reg_526.pbr tad_id: 8 size: 400\n",
      "-- reg_527.pbr tad_id: 8 size: 400\n",
      "-- reg_528.pbr tad_id: 8 size: 400\n",
      "-- reg_529.pbr tad_id: 8 size: 400\n",
      "-- reg_530.pbr tad_id: 8 size: 400\n",
      "-- reg_531.pbr tad_id: 8 size: 400\n",
      "-- reg_532.pbr tad_id: 8 size: 400\n",
      "-- reg_533.pbr tad_id: 8 size: 400\n",
      "-- reg_534.pbr tad_id: 8 size: 400\n",
      "-- reg_535.pbr tad_id: -1 size: 400\n",
      "-- reg_536.pbr tad_id: 9 size: 400\n",
      "-- reg_537.pbr tad_id: 9 size: 400\n",
      "-- reg_538.pbr tad_id: 9 size: 400\n",
      "-- reg_539.pbr tad_id: 9 size: 400\n",
      "-- reg_540.pbr tad_id: 9 size: 400\n",
      "-- reg_541.pbr tad_id: 9 size: 400\n",
      "-- reg_542.pbr tad_id: 9 size: 400\n",
      "-- reg_543.pbr tad_id: 9 size: 400\n",
      "-- reg_544.pbr tad_id: 9 size: 400\n",
      "-- reg_545.pbr tad_id: 9 size: 400\n",
      "-- reg_546.pbr tad_id: 9 size: 400\n",
      "-- reg_547.pbr tad_id: 9 size: 400\n",
      "-- reg_548.pbr tad_id: 9 size: 400\n",
      "-- reg_549.pbr tad_id: 9 size: 400\n",
      "-- reg_550.pbr tad_id: 9 size: 400\n",
      "-- reg_551.pbr tad_id: 9 size: 400\n",
      "-- reg_552.pbr tad_id: 9 size: 400\n",
      "-- reg_553.pbr tad_id: 9 size: 400\n",
      "-- reg_554.pbr tad_id: 9 size: 400\n",
      "-- reg_555.pbr tad_id: 10 size: 400\n",
      "-- reg_556.pbr tad_id: 10 size: 400\n",
      "-- reg_557.pbr tad_id: 10 size: 400\n",
      "-- reg_559.pbr tad_id: 10 size: 216\n",
      "-- reg_560.pbr tad_id: 10 size: 329\n",
      "-- reg_561.pbr tad_id: 10 size: 255\n",
      "-- reg_562.pbr tad_id: 10 size: 400\n",
      "-- reg_563.pbr tad_id: 10 size: 400\n",
      "-- reg_564.pbr tad_id: 10 size: 400\n",
      "-- reg_565.pbr tad_id: 10 size: 400\n",
      "-- reg_566.pbr tad_id: 10 size: 400\n",
      "-- reg_567.pbr tad_id: 10 size: 400\n",
      "-- reg_568.pbr tad_id: 10 size: 400\n",
      "-- reg_569.pbr tad_id: 10 size: 400\n",
      "-- reg_570.pbr tad_id: 10 size: 400\n",
      "-- reg_571.pbr tad_id: 10 size: 400\n",
      "-- reg_572.pbr tad_id: 10 size: 400\n",
      "-- reg_573.pbr tad_id: 10 size: 400\n",
      "-- reg_574.pbr tad_id: 10 size: 400\n",
      "-- reg_575.pbr tad_id: 10 size: 400\n",
      "-- reg_576.pbr tad_id: 10 size: 400\n",
      "-- reg_577.pbr tad_id: 10 size: 400\n",
      "-- reg_578.pbr tad_id: 10 size: 400\n",
      "-- reg_579.pbr tad_id: 10 size: 400\n",
      "-- reg_580.pbr tad_id: 10 size: 400\n",
      "-- reg_581.pbr tad_id: 10 size: 400\n",
      "-- reg_582.pbr tad_id: 11 size: 400\n",
      "-- reg_583.pbr tad_id: 11 size: 400\n",
      "-- reg_584.pbr tad_id: 11 size: 400\n",
      "-- reg_585.pbr tad_id: 11 size: 400\n",
      "-- reg_586.pbr tad_id: 11 size: 400\n",
      "-- reg_587.pbr tad_id: 11 size: 400\n",
      "-- reg_588.pbr tad_id: 11 size: 400\n",
      "-- reg_589.pbr tad_id: -1 size: 400\n",
      "-- reg_590.pbr tad_id: 12 size: 400\n",
      "-- reg_591.pbr tad_id: 12 size: 400\n",
      "-- reg_592.pbr tad_id: 12 size: 400\n",
      "-- reg_593.pbr tad_id: 12 size: 400\n",
      "-- reg_594.pbr tad_id: 12 size: 400\n",
      "-- reg_595.pbr tad_id: 12 size: 400\n",
      "-- reg_596.pbr tad_id: 12 size: 400\n",
      "-- reg_597.pbr tad_id: 12 size: 400\n",
      "-- reg_598.pbr tad_id: 12 size: 400\n",
      "-- reg_599.pbr tad_id: 12 size: 400\n",
      "-- reg_600.pbr tad_id: 12 size: 400\n",
      "-- reg_601.pbr tad_id: 12 size: 400\n",
      "-- reg_602.pbr tad_id: 12 size: 400\n",
      "-- reg_603.pbr tad_id: 12 size: 400\n",
      "-- reg_604.pbr tad_id: 12 size: 400\n",
      "-- reg_605.pbr tad_id: 12 size: 400\n",
      "-- reg_606.pbr tad_id: 12 size: 400\n",
      "-- reg_607.pbr tad_id: 12 size: 400\n",
      "-- reg_608.pbr tad_id: 12 size: 400\n",
      "-- reg_609.pbr tad_id: 12 size: 400\n",
      "-- reg_610.pbr tad_id: 12 size: 400\n",
      "-- reg_611.pbr tad_id: 12 size: 400\n",
      "-- reg_612.pbr tad_id: 12 size: 400\n",
      "-- reg_613.pbr tad_id: 12 size: 400\n",
      "-- reg_614.pbr tad_id: 12 size: 400\n",
      "-- reg_615.pbr tad_id: 12 size: 400\n",
      "-- reg_616.pbr tad_id: 12 size: 400\n",
      "-- reg_617.pbr tad_id: 12 size: 400\n",
      "-- reg_618.pbr tad_id: 12 size: 400\n",
      "-- reg_619.pbr tad_id: 12 size: 400\n",
      "-- reg_620.pbr tad_id: 12 size: 400\n",
      "-- reg_621.pbr tad_id: 12 size: 400\n",
      "-- reg_622.pbr tad_id: -1 size: 400\n",
      "-- reg_623.pbr tad_id: 13 size: 400\n",
      "-- reg_624.pbr tad_id: 13 size: 400\n",
      "-- reg_625.pbr tad_id: 13 size: 400\n",
      "-- reg_626.pbr tad_id: 13 size: 400\n",
      "-- reg_627.pbr tad_id: 13 size: 400\n",
      "-- reg_628.pbr tad_id: 13 size: 400\n",
      "-- reg_629.pbr tad_id: 13 size: 400\n",
      "-- reg_630.pbr tad_id: 13 size: 400\n",
      "-- reg_631.pbr tad_id: 13 size: 400\n",
      "-- reg_632.pbr tad_id: 13 size: 400\n",
      "-- reg_633.pbr tad_id: 13 size: 400\n",
      "-- reg_634.pbr tad_id: 14 size: 400\n",
      "-- reg_635.pbr tad_id: 14 size: 400\n",
      "-- reg_636.pbr tad_id: 14 size: 400\n",
      "-- reg_637.pbr tad_id: 14 size: 400\n",
      "-- reg_638.pbr tad_id: 14 size: 400\n",
      "-- reg_639.pbr tad_id: 14 size: 400\n",
      "-- reg_640.pbr tad_id: 14 size: 400\n",
      "-- reg_641.pbr tad_id: 14 size: 400\n",
      "-- reg_642.pbr tad_id: 14 size: 400\n",
      "-- reg_643.pbr tad_id: 14 size: 400\n",
      "-- reg_644.pbr tad_id: 14 size: 400\n",
      "-- reg_645.pbr tad_id: 14 size: 400\n",
      "-- reg_646.pbr tad_id: 14 size: 400\n",
      "-- reg_647.pbr tad_id: 15 size: 400\n",
      "-- reg_648.pbr tad_id: 15 size: 400\n",
      "-- reg_649.pbr tad_id: 15 size: 400\n",
      "-- reg_650.pbr tad_id: 15 size: 400\n",
      "-- reg_651.pbr tad_id: 15 size: 400\n",
      "-- reg_652.pbr tad_id: 15 size: 400\n",
      "-- reg_653.pbr tad_id: 15 size: 400\n",
      "-- reg_654.pbr tad_id: -1 size: 400\n",
      "-- reg_655.pbr tad_id: -1 size: 400\n",
      "-- reg_656.pbr tad_id: -1 size: 400\n",
      "-- reg_657.pbr tad_id: 16 size: 385\n",
      "-- reg_658.pbr tad_id: 16 size: 400\n",
      "-- reg_659.pbr tad_id: 16 size: 400\n",
      "-- reg_660.pbr tad_id: 16 size: 400\n",
      "-- reg_661.pbr tad_id: 16 size: 400\n",
      "-- reg_662.pbr tad_id: 16 size: 400\n",
      "-- reg_663.pbr tad_id: 16 size: 400\n",
      "-- reg_664.pbr tad_id: 16 size: 400\n",
      "-- reg_665.pbr tad_id: 16 size: 400\n",
      "-- reg_666.pbr tad_id: 16 size: 400\n",
      "-- reg_667.pbr tad_id: 16 size: 400\n",
      "-- reg_668.pbr tad_id: 16 size: 400\n",
      "-- reg_669.pbr tad_id: 16 size: 291\n",
      "-- reg_670.pbr tad_id: 16 size: 213\n",
      "-- reg_671.pbr tad_id: -1 size: 400\n",
      "-- reg_672.pbr tad_id: 17 size: 400\n",
      "-- reg_673.pbr tad_id: 17 size: 400\n",
      "-- reg_674.pbr tad_id: 17 size: 400\n",
      "-- reg_675.pbr tad_id: 17 size: 400\n",
      "-- reg_676.pbr tad_id: 17 size: 400\n",
      "-- reg_677.pbr tad_id: 17 size: 400\n",
      "-- reg_678.pbr tad_id: 17 size: 400\n",
      "-- reg_679.pbr tad_id: 18 size: 400\n",
      "-- reg_680.pbr tad_id: 18 size: 400\n",
      "-- reg_681.pbr tad_id: 18 size: 400\n",
      "-- reg_682.pbr tad_id: 18 size: 400\n",
      "-- reg_683.pbr tad_id: 18 size: 400\n",
      "-- reg_684.pbr tad_id: 18 size: 400\n",
      "-- reg_685.pbr tad_id: 18 size: 400\n",
      "-- reg_686.pbr tad_id: 18 size: 400\n",
      "-- reg_687.pbr tad_id: 18 size: 400\n",
      "-- reg_688.pbr tad_id: 18 size: 400\n",
      "-- reg_689.pbr tad_id: 19 size: 297\n",
      "-- reg_690.pbr tad_id: 19 size: 259\n",
      "-- reg_691.pbr tad_id: 19 size: 400\n",
      "-- reg_692.pbr tad_id: 19 size: 400\n",
      "-- reg_693.pbr tad_id: 19 size: 400\n",
      "-- reg_694.pbr tad_id: 20 size: 400\n",
      "-- reg_695.pbr tad_id: 20 size: 400\n",
      "-- reg_696.pbr tad_id: 20 size: 400\n",
      "-- reg_697.pbr tad_id: 20 size: 400\n",
      "-- reg_698.pbr tad_id: 20 size: 400\n",
      "-- reg_699.pbr tad_id: 20 size: 400\n",
      "-- reg_700.pbr tad_id: 20 size: 400\n",
      "-- reg_701.pbr tad_id: 20 size: 400\n",
      "-- reg_702.pbr tad_id: 20 size: 400\n",
      "-- reg_703.pbr tad_id: 20 size: 400\n",
      "-- reg_704.pbr tad_id: 20 size: 400\n",
      "-- reg_705.pbr tad_id: 20 size: 400\n",
      "-- reg_706.pbr tad_id: 20 size: 400\n",
      "-- reg_707.pbr tad_id: 20 size: 400\n",
      "-- reg_708.pbr tad_id: 20 size: 400\n",
      "-- reg_709.pbr tad_id: 20 size: 400\n",
      "-- reg_710.pbr tad_id: 20 size: 400\n",
      "-- reg_711.pbr tad_id: 20 size: 400\n",
      "-- reg_712.pbr tad_id: 20 size: 400\n",
      "-- reg_713.pbr tad_id: 20 size: 400\n",
      "-- reg_714.pbr tad_id: 20 size: 400\n",
      "-- reg_715.pbr tad_id: 20 size: 400\n",
      "-- reg_716.pbr tad_id: 20 size: 400\n",
      "-- reg_717.pbr tad_id: 20 size: 400\n",
      "-- reg_718.pbr tad_id: 20 size: 400\n",
      "-- reg_719.pbr tad_id: 20 size: 400\n",
      "-- reg_720.pbr tad_id: 20 size: 400\n",
      "-- reg_721.pbr tad_id: 20 size: 400\n",
      "-- reg_722.pbr tad_id: 20 size: 400\n",
      "-- reg_723.pbr tad_id: 20 size: 395\n",
      "-- reg_724.pbr tad_id: 21 size: 400\n",
      "-- reg_725.pbr tad_id: 21 size: 400\n",
      "-- reg_726.pbr tad_id: 21 size: 400\n",
      "-- reg_727.pbr tad_id: 21 size: 330\n",
      "-- reg_728.pbr tad_id: -1 size: 400\n",
      "-- reg_729.pbr tad_id: 22 size: 400\n",
      "-- reg_730.pbr tad_id: 22 size: 400\n",
      "-- reg_731.pbr tad_id: 22 size: 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- reg_732.pbr tad_id: 22 size: 400\n",
      "-- reg_733.pbr tad_id: 22 size: 400\n",
      "-- reg_734.pbr tad_id: 22 size: 400\n",
      "-- reg_735.pbr tad_id: 22 size: 400\n",
      "-- reg_736.pbr tad_id: 22 size: 400\n",
      "-- reg_737.pbr tad_id: 22 size: 400\n",
      "-- reg_738.pbr tad_id: 22 size: 400\n",
      "-- reg_739.pbr tad_id: 22 size: 400\n",
      "-- reg_740.pbr tad_id: 22 size: 400\n",
      "-- reg_741.pbr tad_id: 22 size: 400\n",
      "-- reg_742.pbr tad_id: 22 size: 400\n",
      "-- reg_743.pbr tad_id: 22 size: 400\n",
      "-- reg_744.pbr tad_id: 23 size: 400\n",
      "-- reg_745.pbr tad_id: 23 size: 400\n",
      "-- reg_746.pbr tad_id: 23 size: 400\n",
      "-- reg_747.pbr tad_id: 23 size: 329\n",
      "-- reg_748.pbr tad_id: 23 size: 400\n",
      "-- reg_749.pbr tad_id: 23 size: 400\n",
      "-- reg_750.pbr tad_id: 23 size: 400\n",
      "-- reg_751.pbr tad_id: 23 size: 400\n",
      "-- reg_752.pbr tad_id: 23 size: 400\n",
      "-- reg_753.pbr tad_id: -1 size: 400\n",
      "-- reg_754.pbr tad_id: 24 size: 400\n",
      "-- reg_755.pbr tad_id: 24 size: 400\n",
      "-- reg_756.pbr tad_id: 24 size: 400\n",
      "-- reg_757.pbr tad_id: 24 size: 400\n",
      "-- reg_758.pbr tad_id: 24 size: 400\n",
      "-- reg_759.pbr tad_id: 24 size: 400\n",
      "-- reg_760.pbr tad_id: 24 size: 400\n",
      "-- reg_761.pbr tad_id: 24 size: 400\n",
      "-- reg_762.pbr tad_id: 24 size: 400\n",
      "-- reg_763.pbr tad_id: 24 size: 400\n",
      "-- reg_764.pbr tad_id: 24 size: 400\n",
      "-- reg_765.pbr tad_id: 24 size: 400\n",
      "-- reg_766.pbr tad_id: 24 size: 400\n",
      "-- reg_767.pbr tad_id: 24 size: 400\n",
      "-- reg_768.pbr tad_id: 24 size: 400\n",
      "-- reg_769.pbr tad_id: 24 size: 400\n",
      "-- reg_770.pbr tad_id: 25 size: 400\n",
      "-- reg_771.pbr tad_id: 25 size: 400\n",
      "-- reg_772.pbr tad_id: 25 size: 400\n",
      "-- reg_773.pbr tad_id: 25 size: 400\n",
      "-- reg_774.pbr tad_id: 25 size: 400\n",
      "-- reg_775.pbr tad_id: 25 size: 400\n",
      "-- reg_776.pbr tad_id: 25 size: 400\n",
      "-- reg_777.pbr tad_id: 25 size: 400\n",
      "-- reg_778.pbr tad_id: 25 size: 400\n",
      "-- reg_779.pbr tad_id: 25 size: 400\n",
      "-- reg_780.pbr tad_id: 25 size: 400\n",
      "-- reg_781.pbr tad_id: 25 size: 400\n",
      "-- reg_782.pbr tad_id: 25 size: 400\n",
      "-- reg_783.pbr tad_id: 25 size: 400\n",
      "-- reg_784.pbr tad_id: 25 size: 400\n",
      "-- reg_785.pbr tad_id: 25 size: 400\n",
      "-- reg_786.pbr tad_id: 25 size: 400\n",
      "-- reg_787.pbr tad_id: 25 size: 400\n",
      "-- reg_788.pbr tad_id: 25 size: 400\n",
      "-- reg_789.pbr tad_id: 25 size: 400\n",
      "-- reg_790.pbr tad_id: 26 size: 400\n",
      "-- reg_791.pbr tad_id: 26 size: 400\n",
      "-- reg_792.pbr tad_id: 26 size: 400\n",
      "-- reg_793.pbr tad_id: 26 size: 400\n",
      "-- reg_794.pbr tad_id: 26 size: 400\n",
      "-- reg_795.pbr tad_id: 26 size: 400\n",
      "-- reg_796.pbr tad_id: 26 size: 400\n",
      "-- reg_797.pbr tad_id: 26 size: 400\n",
      "-- reg_798.pbr tad_id: 26 size: 400\n",
      "-- reg_799.pbr tad_id: 26 size: 400\n",
      "-- reg_800.pbr tad_id: 26 size: 400\n",
      "-- reg_801.pbr tad_id: 26 size: 400\n",
      "-- reg_802.pbr tad_id: 26 size: 400\n",
      "-- reg_803.pbr tad_id: 26 size: 400\n",
      "-- reg_804.pbr tad_id: 26 size: 400\n",
      "-- reg_805.pbr tad_id: 26 size: 400\n",
      "-- reg_806.pbr tad_id: 26 size: 400\n",
      "-- reg_807.pbr tad_id: 26 size: 400\n",
      "-- reg_808.pbr tad_id: 26 size: 400\n",
      "-- reg_809.pbr tad_id: 26 size: 400\n",
      "-- reg_810.pbr tad_id: 26 size: 400\n",
      "-- reg_811.pbr tad_id: 26 size: 400\n",
      "-- reg_812.pbr tad_id: 26 size: 400\n",
      "-- reg_813.pbr tad_id: 26 size: 400\n",
      "-- reg_814.pbr tad_id: 26 size: 400\n",
      "-- reg_815.pbr tad_id: 26 size: 400\n",
      "-- reg_816.pbr tad_id: 26 size: 400\n",
      "-- reg_817.pbr tad_id: 26 size: 400\n",
      "-- reg_818.pbr tad_id: 26 size: 400\n",
      "-- reg_819.pbr tad_id: 26 size: 400\n",
      "-- reg_820.pbr tad_id: 26 size: 400\n",
      "-- reg_821.pbr tad_id: 26 size: 400\n",
      "-- reg_822.pbr tad_id: 26 size: 400\n",
      "-- reg_823.pbr tad_id: 26 size: 400\n",
      "-- reg_824.pbr tad_id: 27 size: 400\n",
      "-- reg_825.pbr tad_id: 27 size: 400\n",
      "-- reg_826.pbr tad_id: 27 size: 400\n",
      "-- reg_827.pbr tad_id: 27 size: 400\n",
      "-- reg_828.pbr tad_id: 27 size: 400\n",
      "-- reg_829.pbr tad_id: 27 size: 400\n",
      "-- reg_830.pbr tad_id: 27 size: 400\n",
      "-- reg_831.pbr tad_id: 27 size: 400\n",
      "-- reg_832.pbr tad_id: 27 size: 400\n",
      "-- reg_833.pbr tad_id: 27 size: 400\n",
      "-- reg_834.pbr tad_id: 27 size: 400\n",
      "-- reg_835.pbr tad_id: 27 size: 400\n",
      "-- reg_836.pbr tad_id: 27 size: 400\n",
      "-- reg_837.pbr tad_id: 27 size: 400\n",
      "-- reg_838.pbr tad_id: 28 size: 400\n",
      "-- reg_839.pbr tad_id: 28 size: 400\n",
      "-- reg_840.pbr tad_id: 28 size: 400\n",
      "-- reg_841.pbr tad_id: 28 size: 400\n",
      "-- reg_842.pbr tad_id: 28 size: 400\n",
      "-- reg_843.pbr tad_id: 28 size: 400\n",
      "-- reg_844.pbr tad_id: 28 size: 400\n",
      "-- reg_845.pbr tad_id: 28 size: 400\n",
      "-- reg_846.pbr tad_id: 28 size: 400\n",
      "-- reg_847.pbr tad_id: 28 size: 400\n",
      "-- reg_848.pbr tad_id: 28 size: 400\n",
      "-- reg_849.pbr tad_id: 28 size: 400\n",
      "-- reg_850.pbr tad_id: 28 size: 400\n",
      "-- reg_851.pbr tad_id: 28 size: 400\n",
      "-- reg_852.pbr tad_id: 29 size: 400\n",
      "-- reg_853.pbr tad_id: 29 size: 400\n",
      "-- reg_854.pbr tad_id: 29 size: 400\n",
      "-- reg_855.pbr tad_id: 29 size: 400\n",
      "-- reg_856.pbr tad_id: 29 size: 400\n",
      "-- reg_857.pbr tad_id: 29 size: 400\n",
      "-- reg_858.pbr tad_id: 30 size: 400\n",
      "-- reg_859.pbr tad_id: 30 size: 366\n",
      "-- reg_860.pbr tad_id: 30 size: 372\n",
      "-- reg_861.pbr tad_id: 30 size: 400\n",
      "-- reg_862.pbr tad_id: 30 size: 400\n",
      "-- reg_863.pbr tad_id: 30 size: 278\n",
      "-- reg_864.pbr tad_id: 30 size: 400\n",
      "-- reg_866.pbr tad_id: 30 size: 400\n",
      "-- reg_867.pbr tad_id: 30 size: 400\n",
      "-- reg_868.pbr tad_id: 30 size: 400\n",
      "-- reg_869.pbr tad_id: 30 size: 400\n",
      "-- reg_870.pbr tad_id: 30 size: 400\n",
      "-- reg_871.pbr tad_id: 30 size: 319\n",
      "-- reg_872.pbr tad_id: 30 size: 400\n",
      "-- reg_873.pbr tad_id: 30 size: 400\n",
      "-- reg_874.pbr tad_id: 30 size: 400\n",
      "-- reg_875.pbr tad_id: 30 size: 400\n",
      "-- reg_876.pbr tad_id: 30 size: 400\n",
      "-- reg_877.pbr tad_id: 31 size: 400\n",
      "-- reg_878.pbr tad_id: 31 size: 400\n",
      "-- reg_879.pbr tad_id: 31 size: 400\n",
      "-- reg_880.pbr tad_id: 31 size: 400\n",
      "-- reg_881.pbr tad_id: 31 size: 400\n",
      "-- reg_882.pbr tad_id: 31 size: 400\n",
      "-- reg_883.pbr tad_id: 32 size: 400\n",
      "-- reg_884.pbr tad_id: 32 size: 400\n",
      "-- reg_885.pbr tad_id: 32 size: 400\n",
      "-- reg_886.pbr tad_id: 32 size: 400\n",
      "-- reg_887.pbr tad_id: 32 size: 400\n",
      "-- reg_888.pbr tad_id: 32 size: 400\n",
      "-- reg_889.pbr tad_id: 32 size: 400\n",
      "-- reg_890.pbr tad_id: 32 size: 400\n",
      "-- reg_891.pbr tad_id: 32 size: 400\n",
      "-- reg_892.pbr tad_id: 32 size: 400\n",
      "-- reg_893.pbr tad_id: 32 size: 400\n",
      "-- reg_894.pbr tad_id: 32 size: 400\n",
      "-- reg_895.pbr tad_id: 32 size: 400\n",
      "-- reg_896.pbr tad_id: 32 size: 400\n",
      "-- reg_897.pbr tad_id: 33 size: 400\n",
      "-- reg_898.pbr tad_id: 33 size: 400\n",
      "-- reg_899.pbr tad_id: 33 size: 400\n",
      "-- reg_900.pbr tad_id: 33 size: 400\n",
      "-- reg_901.pbr tad_id: 33 size: 400\n",
      "-- reg_902.pbr tad_id: 33 size: 400\n",
      "-- reg_903.pbr tad_id: 33 size: 400\n",
      "-- reg_904.pbr tad_id: -1 size: 400\n",
      "-- reg_905.pbr tad_id: 34 size: 400\n",
      "-- reg_906.pbr tad_id: 34 size: 400\n",
      "-- reg_907.pbr tad_id: 34 size: 400\n",
      "-- reg_908.pbr tad_id: 34 size: 400\n",
      "-- reg_909.pbr tad_id: 34 size: 400\n",
      "-- reg_910.pbr tad_id: 34 size: 400\n",
      "-- reg_911.pbr tad_id: 34 size: 400\n",
      "-- reg_912.pbr tad_id: 34 size: 400\n",
      "-- reg_913.pbr tad_id: 34 size: 400\n",
      "-- reg_914.pbr tad_id: 34 size: 400\n",
      "-- reg_915.pbr tad_id: 34 size: 400\n",
      "-- reg_916.pbr tad_id: 34 size: 400\n",
      "-- reg_917.pbr tad_id: 34 size: 400\n",
      "-- reg_918.pbr tad_id: 34 size: 400\n",
      "-- reg_919.pbr tad_id: 35 size: 400\n",
      "-- reg_920.pbr tad_id: 35 size: 400\n",
      "-- reg_921.pbr tad_id: 35 size: 400\n",
      "-- reg_922.pbr tad_id: 35 size: 400\n",
      "-- reg_923.pbr tad_id: 35 size: 400\n",
      "-- reg_924.pbr tad_id: 35 size: 340\n",
      "-- reg_925.pbr tad_id: 35 size: 400\n",
      "-- reg_926.pbr tad_id: 35 size: 400\n",
      "-- reg_927.pbr tad_id: 35 size: 400\n",
      "-- reg_928.pbr tad_id: 35 size: 400\n",
      "-- reg_929.pbr tad_id: 35 size: 400\n",
      "-- reg_930.pbr tad_id: 35 size: 400\n",
      "-- reg_931.pbr tad_id: 35 size: 400\n",
      "-- reg_932.pbr tad_id: 35 size: 400\n",
      "-- reg_933.pbr tad_id: 35 size: 400\n",
      "-- reg_934.pbr tad_id: 35 size: 400\n"
     ]
    }
   ],
   "source": [
    "master_folder = r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36';\n",
    "report_folder = r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400'; # if merged\n",
    "\n",
    "tad_dics, tad_id_dic, reg_len_dic, new_id_dic= Match_TADs(master_folder,\n",
    "                                                          TAD_ref='chr21_TADs_splitted.bed', \n",
    "                                                          report_folder=report_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Assign color and cluster id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design from subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sub_encodings for chr21 small\n"
     ]
    }
   ],
   "source": [
    "# dic for chr21 small sub-encoding scheme\n",
    "import cPickle as pickle\n",
    "print 'loading sub_encodings for chr21 small'\n",
    "chr21_sub_encodings = pickle.load(open(r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/sub_encoding.pkl','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can continue here!\n",
    "region_folder = r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36';\n",
    "save_folder = r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36';\n",
    "\n",
    "# dic for region -> tad\n",
    "if not 'new_id_dic' in vars():\n",
    "    import cPickle as pickle\n",
    "    print \"-- loading reg-tad-dic\"\n",
    "    new_id_dic = pickle.load(open(region_folder+os.sep+'region_to_TAD.pkl','r'))\n",
    "\n",
    "sub_reg_id_dic = {};\n",
    "for k,v in sorted(new_id_dic.items()):\n",
    "    if k in chr21_sub_encodings.keys():\n",
    "        sub_reg_id_dic[k] = v;\n",
    "\n",
    "# dic for region -> it's length\n",
    "if not 'reg_len_dic' in vars():\n",
    "    import cPickle as pickle\n",
    "    print \"-- loading reg-size-dic\"\n",
    "    reg_len_dic = pickle.load(open(region_folder+os.sep+'region_length.pkl','r'))\n",
    "\n",
    "sub_reg_size_dic = {};\n",
    "for k,v in sorted(reg_len_dic.items()):\n",
    "    if k in chr21_sub_encodings.keys():\n",
    "        sub_reg_size_dic[k] = v;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Design_Encoding(reg_id_dic, reg_size_dic, size_threshold=200,\n",
    "                    n_color=3,\n",
    "                    n_reg=10, n_hyb=5, min_region_times=2,\n",
    "                    filling_rows=True,\n",
    "                    save=True, save_folder=r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21',\n",
    "                    verbose=True):\n",
    "    '''Design encoding scheme\n",
    "    Inputs:\n",
    "        reg_id_dic: region -> TAD dictionary, dic\n",
    "        reg_size_dic: region -> number of probe dictionary, dic\n",
    "        size_threshold: lower bound for number of probes in each region, int\n",
    "        n_color: number of colors, int\n",
    "        n_reg: number of region per decoding unit, int\n",
    "        n_hyb: number of hybes per decoding unit, int\n",
    "        min_region_times: minimum region appearing times, int\n",
    "        filling_rows: whether I should fill remaining region into last row, bool\n",
    "        save: whether save final result, bool\n",
    "        save_folder: save directory, string\n",
    "        verbose: whether say something!, bool\n",
    "    Output:\n",
    "        reg_encoding: region_number -> color=i, cluster=j, region=k, barcodes->...\n",
    "        hyb_matrix: hybridization matrix, n_reg by n_hyb\n",
    "        assign_regs: matrix of assigning region to clusters, n_color by n_cluster by n_reg\n",
    "        assign_tads: matrix of assigning tad to clusters, n_color by n_cluster by n_reg\n",
    "        '''\n",
    "    # imports\n",
    "    import numpy as np;\n",
    "    \n",
    "    def _TAD_to_Region(reg_id_dic, _reg_size_dic=reg_size_dic, _size_threshold=size_threshold, _verbose=verbose):\n",
    "        '''Function to inverse region->TAD dictionary'''\n",
    "        if _verbose:\n",
    "            print '-- Converting region->TAD dic into TAD->[regions]';\n",
    "            \n",
    "        _tad_to_region = {}\n",
    "        for k, v in reg_id_dic.iteritems():\n",
    "            if value >= 0 and _reg_size_dic[k] > _size_threshold:\n",
    "                _tad_to_region[v] = _tad_to_region.get(v, [])\n",
    "                _tad_to_region[v].append(k)   \n",
    "        _tad_to_region.pop(-1, None);\n",
    "        \n",
    "        if _verbose:\n",
    "            for k,v in sorted(_tad_to_region.items()):\n",
    "                print '---TAD: '+str(k);\n",
    "                print v;\n",
    "                \n",
    "        \n",
    "        return _tad_to_region;\n",
    "\n",
    "    def _Generate_Hyb_Matrix(n_reg=n_reg, n_hyb=n_hyb, min_region_times=min_region_times, _verbose=verbose):\n",
    "        '''Function to generate hybridization matrix\n",
    "        Input: number of regions\n",
    "               number of hybridizations\n",
    "               the minimal time that each region appears. default:1\n",
    "        Output: A hybridization matrix'''\n",
    "        if _verbose:\n",
    "            print '-- Generating hybridization matrix for region='+str(n_reg)+', hyb='+str(n_hyb);        \n",
    "        \n",
    "        # generate all possible all_codess\n",
    "        all_codes =[] # list for all possible binary all_codess\n",
    "        for i in range(2**n_hyb):\n",
    "            hybe_0 = np.zeros(n_hyb,dtype=int)\n",
    "            binrep = [int(c) for c in str(\"{0:#b}\".format(i))[2:]]\n",
    "            #print str(\"{0:#b}\".format(i))[2:]\n",
    "            hybe_0[-len(binrep):]=binrep\n",
    "            all_codes.append(hybe_0)\n",
    "        all_codes = np.array(all_codes)\n",
    "        all_codes = all_codes[np.sum(all_codes,-1)>0]\n",
    "        # Choose candicate codes\n",
    "        _code_sums = np.sum(all_codes,axis=-1) \n",
    "        _code_sums[_code_sums < min_region_times]=np.max(_code_sums)+1 # remove codes that dont satisfy minimal region showup times\n",
    "        _max_region_time = np.sort(_code_sums)[n_reg] # maximum region appearance\n",
    "        if min_region_times == _max_region_time: # Case 1: all regions has the same code\n",
    "            _nchoose = n_reg\n",
    "            _cand_codes = all_codes[_code_sums == _max_region_time];\n",
    "            _sims = []\n",
    "            for _i in range(20000):\n",
    "                _sim = _cand_codes[np.random.choice(range(len(_cand_codes)), _nchoose, replace=False)]\n",
    "                _sims.append(_sim)\n",
    "            _sim_keep = _sims[np.argmin([np.var(np.sum(_sim,axis=0)) for _sim in _sims])]\n",
    "            _hyb_matrix = np.array(list(_sim_keep))\n",
    "        else:  # Case 2: use lower-choose codes first, and then use higher codes\n",
    "            _used_codes = list(all_codes[_code_sums < _max_region_time]) # use up all shorter codes\n",
    "            _nchoose = n_reg-len(_used_codes) # other codes to be chosen\n",
    "            _cand_codes = all_codes[_code_sums == _max_region_time]\n",
    "            _sims = []\n",
    "            for _i in range(20000):\n",
    "                _sim = _cand_codes[np.random.choice(range(len(_cand_codes)), _nchoose, replace=False)]\n",
    "                _sims.append(_sim)\n",
    "            _sim_keep = _sims[np.argmin([np.var(np.sum(_sim,axis=0)) for _sim in _sims])]\n",
    "            _used_codes+=list(_sim_keep)\n",
    "            _hyb_matrix = np.array(_used_codes).astype(np.int)\n",
    "\n",
    "        return _hyb_matrix\n",
    "    \n",
    "    def _Assign_Color(_reg_encodings, _tad_to_region, _n_color=n_color, _verbose=verbose):\n",
    "        if _verbose:\n",
    "            print '-- Assigning colors for all regions';\n",
    "        _reg_colors = [[] for _color in range(_n_color)]\n",
    "        _mode_counter = 0; # used for balancing mode_n results into n categories\n",
    "        for _k,_v in _tad_to_region.iteritems():\n",
    "            for _color in range(_n_color):\n",
    "                _reg_list = _v[(_mode_counter+_color)%_n_color::_n_color];\n",
    "                _reg_colors[_color].append(_reg_list);\n",
    "                for _reg in _reg_list:\n",
    "                    _reg_encodings[_reg]['color'] = _color\n",
    "            _mode_counter += 1;\n",
    "        if _verbose:\n",
    "            for _color in range(_n_color):\n",
    "                lstlen=0\n",
    "                for lst in _reg_colors[_color]:\n",
    "                    lstlen += len(lst)\n",
    "                print '--- Number of regions in color '+str(_color)+':', lstlen\n",
    "        return _reg_encodings, _reg_colors;\n",
    "\n",
    "    def _Assign_Cluster(reg_encodings, reg_colors, n_reg=n_reg, n_color=n_color, \n",
    "                        _filling_rows=filling_rows, _verbose=verbose):\n",
    "        '''Assign regions into clusters'''\n",
    "        from math import ceil\n",
    "        from copy import copy\n",
    "        if _verbose:\n",
    "            print '-- Assigning clusters for all regions';\n",
    "            \n",
    "        # calculate number of clusters in each color\n",
    "        n_cluster = int(ceil(len(reg_encodings)/float(n_color*n_reg)))\n",
    "        # initialize matrix\n",
    "        _assign_regs = -np.ones([n_color, n_cluster, n_reg],dtype=np.int)\n",
    "        # vector to store how many clusters being assgined;\n",
    "        _assigned_cluster_num = [];\n",
    "        for _color in range(n_color):\n",
    "            _rlist = copy(sorted(reg_colors[_color],key=lambda v:-len(v)));\n",
    "            _cluster = 0;\n",
    "            while len(_rlist) >= n_reg:\n",
    "                for _reg in range(n_reg):\n",
    "                    _assign_regs[_color, _cluster, _reg] = _rlist[_reg].pop(0)\n",
    "                    # store into reg_encodings\n",
    "                    reg_encodings[_assign_regs[_color, _cluster, _reg]]['color'] = _color;\n",
    "                    reg_encodings[_assign_regs[_color, _cluster, _reg]]['cluster'] = _cluster;\n",
    "                    reg_encodings[_assign_regs[_color, _cluster, _reg]]['region'] = _reg;\n",
    "                # clean all empty lists\n",
    "                while [] in _rlist:\n",
    "                    _rlist.remove([]);\n",
    "                # sort again\n",
    "                _rlist = sorted(_rlist, key=lambda v:-len(v));\n",
    "                # next cluster\n",
    "                _cluster += 1\n",
    "            # for the left regions, store then in the last row\n",
    "            if _filling_rows:\n",
    "                _assign_regs[_color, _cluster, :len(_rlist)] = np.array(_rlist).reshape(-1) # store the rest\n",
    "                _cluster += 1;\n",
    "                for _reg in range(len(_rlist)):\n",
    "                    reg_encodings[_assign_regs[_color, _cluster, _reg]]['color'] = _color;\n",
    "                    reg_encodings[_assign_regs[_color, _cluster, _reg]]['cluster'] = _cluster;\n",
    "                    reg_encodings[_assign_regs[_color, _cluster, _reg]]['region'] = _reg;    \n",
    "            else:\n",
    "                print '-- region without decoding_barcode:',_rlist;\n",
    "                _left_regs = sum(_rlist,[]);\n",
    "                for _reg in _left_regs:\n",
    "                    reg_encodings[_reg]['color'] = _color;\n",
    "                    reg_encodings[_reg]['cluster'] = None;\n",
    "                    reg_encodings[_reg]['region'] = None;   \n",
    "            _assigned_cluster_num.append(_cluster);\n",
    "            \n",
    "        # Trim _assign_regs if not filling rows:\n",
    "        if not filling_rows:\n",
    "            print \"Number of clusters in each color:\\n\", _assigned_cluster_num;\n",
    "            _assign_regs = _assign_regs[:,:max(_assigned_cluster_num),:];\n",
    "            \n",
    "            \n",
    "        return reg_encodings, _assign_regs;\n",
    "    \n",
    "    def _Assign_Decoding_Barcodes(reg_encodings, assign_regs, hyb_matrix,\n",
    "                                  n_color=n_color, n_reg=n_reg, n_hyb=n_hyb, _verbose=verbose):\n",
    "        '''Assign barcode (orders) used for decoding'''\n",
    "        if _verbose:\n",
    "            print '-- Assigning decoding barcodes.'        \n",
    "        # Sanity check\n",
    "        if np.shape(assign_regs)[0] != n_color or np.shape(assign_regs)[2] != n_reg:\n",
    "            raise EOFError('wrong input dimension!');\n",
    "        # collect number of clusters per color\n",
    "        n_cluster = np.shape(assign_regs)[1];\n",
    "        _barcode_set = 0; # barcode to be assigned\n",
    "        for _color in range(n_color):\n",
    "            for _cluster in range(n_cluster):\n",
    "                if list(assign_regs[_color,_cluster,:]).count(-1) == len(list(assign_regs[_color,_cluster,:])): # if all regions in this cluster unassigned\n",
    "                    print 'pass'\n",
    "                    continue;\n",
    "                for _reg in range(n_reg):\n",
    "                    if assign_regs[_color,_cluster,_reg] >= 0:\n",
    "                        reg_encodings[assign_regs[_color,_cluster,_reg]]['bc_decoding'] = [n_hyb*_barcode_set+ i for i, j in enumerate(hyb_matrix[_reg]) if j == 1]\n",
    "                _barcode_set += 1; # next barcode set (size of n_hyb)\n",
    "        return reg_encodings;\n",
    "    \n",
    "    def _Check_Decoding_Barcodes(reg_encodings, hyb_matrix, _verbose=verbose):\n",
    "        '''Function to check whether decoding barcode works fine'''\n",
    "        if _verbose:\n",
    "            print '--- Checking decoding barcodes.'  \n",
    "        reg_bc_num=hyb_matrix.sum(1).max()\n",
    "        hyb_bc_num=hyb_matrix.sum(0).max()   \n",
    "        bc_list = [];\n",
    "        for k,v in reg_encodings.iteritems():\n",
    "            if v['bc_decoding'] != None:\n",
    "                if len(v['bc_decoding']) > reg_bc_num or len(v['bc_decoding']) <=0:\n",
    "                    print '--- wrong barcode size per region';\n",
    "                    return False\n",
    "                bc_list += v['bc_decoding'];\n",
    "        # record unique barcodes\n",
    "        barcodes, barcode_counts = np.unique(bc_list, return_counts=True)\n",
    "        print barcodes\n",
    "        # check barcode usage per hybe\n",
    "        validate = False not in [n<=hyb_bc_num and n>0 for n in barcode_counts]\n",
    "        print '---', validate\n",
    "        return validate\n",
    "\n",
    "    def _Assign_TAD_Barcodes(reg_encodings, _verbose=verbose):\n",
    "        '''Assign barcode (orders) used for TAD identity'''\n",
    "        if _verbose:\n",
    "            print '-- Assigning TAD barcodes.' \n",
    "        # record all decoding barcodes\n",
    "        dec_bcs = []\n",
    "        for k,v in reg_encodings.iteritems():\n",
    "            if v['bc_decoding'] != None:\n",
    "                dec_bcs += v['bc_decoding']\n",
    "        # tad barcodes should start right after\n",
    "        tad_bc_start = max(dec_bcs)+1; \n",
    "        for k,v in reg_encodings.iteritems():\n",
    "            if v['TAD']>=0:\n",
    "                reg_encodings[k]['bc_tad'] = reg_encodings[k]['TAD'] + tad_bc_start;\n",
    "        \n",
    "        return reg_encodings\n",
    "        \n",
    "            \n",
    "    def _Assign_Unique_Barcodes(reg_encodings, _verbose=verbose):\n",
    "        '''Assign barcode (orders) used for unique sequential'''\n",
    "        if _verbose:\n",
    "            print '-- Assigning unique barcodes.'\n",
    "        # record all decoding barcodes and TAD barcodes\n",
    "        used_bcs = []\n",
    "        for k,v in reg_encodings.iteritems():\n",
    "            if v['bc_decoding'] != None:\n",
    "                used_bcs += v['bc_decoding']\n",
    "            used_bcs += [v['bc_tad']]\n",
    "        # unique barcodes should start right after\n",
    "        unique_bc_start = max(used_bcs); \n",
    "        reg_new_id = 1;\n",
    "        for k,v in sorted(reg_encodings.items()):\n",
    "            reg_encodings[k]['bc_unique'] = reg_new_id + unique_bc_start;\n",
    "            reg_encodings[k]['id'] = reg_new_id;\n",
    "            reg_new_id += 1;\n",
    "        \n",
    "        return reg_encodings  \n",
    "    \n",
    "    \n",
    "    # Initialize\n",
    "    if verbose:\n",
    "        print \"- Initializing\";\n",
    "    reg_encodings = {};\n",
    "    for key, value in reg_id_dic.items():\n",
    "        if value >= 0 and reg_size_dic[key] >= size_threshold: \n",
    "            reg_encodings[key] = {'TAD':value, 'id':None, 'color':None, \\\n",
    "                                  'cluster':None, 'region': None, \\\n",
    "                                  'bc_decoding':None,\\\n",
    "                                  'bc_tad':None, 'bc_unique':None}\n",
    "    \n",
    "\n",
    "    # creat tad to region dictionary\n",
    "    if verbose:\n",
    "        print \"- Inverting region_to_tad dictionary\";\n",
    "    tad_to_region = _TAD_to_Region(reg_id_dic);\n",
    "    \n",
    "    # generate hybe matrix\n",
    "    if verbose:\n",
    "        print \"- Prepare hyb matrix\";\n",
    "    hyb_matrix = _Generate_Hyb_Matrix()\n",
    "    \n",
    "    if verbose:\n",
    "        print \"- Calculate color, cluster assignment\";    \n",
    "    # assign colors\n",
    "    reg_encodings , reg_colors = _Assign_Color(reg_encodings, tad_to_region);\n",
    "    # assign cluster\n",
    "    reg_encodings, assign_regs = _Assign_Cluster(reg_encodings, reg_colors);\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print \"- Assign barcodes\";    \n",
    "    # assign decoding barcodes\n",
    "    reg_encodings = _Assign_Decoding_Barcodes(reg_encodings, assign_regs, hyb_matrix)\n",
    "    # check decoding barcodes\n",
    "    decoding_check = _Check_Decoding_Barcodes(reg_encodings, hyb_matrix)\n",
    "    # assign TAD barcodes\n",
    "    reg_encodings = _Assign_TAD_Barcodes(reg_encodings)\n",
    "    # assign unique barcodes\n",
    "    reg_encodings = _Assign_Unique_Barcodes(reg_encodings)    \n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        import cPickle as pickle\n",
    "        import os\n",
    "        # mkdir if not exist for save folder\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        save_filename = save_folder + os.sep + 'total_encoding.pkl';\n",
    "        if verbose:\n",
    "            print \"- Save to file:\", save_filename\n",
    "        savefile = open(save_filename, 'w');\n",
    "        pickle.dump(reg_encodings, savefile)\n",
    "        \n",
    "    return reg_encodings, hyb_matrix, assign_regs\n",
    "\n",
    "    \n",
    "def Design_Noncoding_Sequential(reg_id_dic, reg_size_dic, threshold=200,\n",
    "                                n_color=3, save=True, verbose=True):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Initializing\n",
      "- Inverting region_to_tad dictionary\n",
      "-- Converting region->TAD dic into TAD->[regions]\n",
      "---TAD: 0\n",
      "[209, 211, 213]\n",
      "---TAD: 1\n",
      "[266, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317]\n",
      "---TAD: 2\n",
      "[318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360]\n",
      "---TAD: 3\n",
      "[361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388]\n",
      "---TAD: 4\n",
      "[389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418]\n",
      "---TAD: 5\n",
      "[419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 446, 447, 448, 449, 450]\n",
      "---TAD: 6\n",
      "[451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481]\n",
      "---TAD: 7\n",
      "[482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513]\n",
      "---TAD: 8\n",
      "[514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534]\n",
      "---TAD: 9\n",
      "[535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554]\n",
      "---TAD: 10\n",
      "[555, 556, 557, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581]\n",
      "---TAD: 11\n",
      "[582, 583, 584, 585, 586, 587, 588, 589]\n",
      "---TAD: 12\n",
      "[590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621]\n",
      "---TAD: 13\n",
      "[622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633]\n",
      "---TAD: 14\n",
      "[634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646]\n",
      "---TAD: 15\n",
      "[647, 648, 649, 650, 651, 652, 653, 654, 655]\n",
      "---TAD: 16\n",
      "[656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670]\n",
      "---TAD: 17\n",
      "[671, 672, 673, 674, 675, 676, 677, 678]\n",
      "---TAD: 18\n",
      "[679, 680, 681, 682, 683, 684, 685, 686, 687, 688]\n",
      "---TAD: 19\n",
      "[689, 690, 691, 692, 693]\n",
      "---TAD: 20\n",
      "[694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723]\n",
      "---TAD: 21\n",
      "[724, 725, 726, 727, 728]\n",
      "---TAD: 22\n",
      "[729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743]\n",
      "---TAD: 23\n",
      "[744, 745, 746, 747, 748, 749, 750, 751, 752, 753]\n",
      "---TAD: 24\n",
      "[754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769]\n",
      "---TAD: 25\n",
      "[770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789]\n",
      "---TAD: 26\n",
      "[790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823]\n",
      "---TAD: 27\n",
      "[824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837]\n",
      "---TAD: 28\n",
      "[838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851]\n",
      "---TAD: 29\n",
      "[852, 853, 854, 855, 856, 857]\n",
      "---TAD: 30\n",
      "[858, 859, 860, 861, 862, 863, 864, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876]\n",
      "---TAD: 31\n",
      "[877, 878, 879, 880, 881, 882]\n",
      "---TAD: 32\n",
      "[883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896]\n",
      "---TAD: 33\n",
      "[897, 898, 899, 900, 901, 902, 903, 904]\n",
      "---TAD: 34\n",
      "[905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918]\n",
      "---TAD: 35\n",
      "[919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934]\n",
      "- Prepare hyb matrix\n",
      "-- Generating hybridization matrix for region=36, hyb=9\n",
      "- Calculate color, cluster assignment\n",
      "-- Assigning colors for all regions\n",
      "--- Number of regions in color 0: 220\n",
      "--- Number of regions in color 1: 217\n",
      "--- Number of regions in color 2: 214\n",
      "-- Assigning clusters for all regions\n",
      "-- region without decoding_barcode: [[284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317], [323, 326, 329, 332, 335, 338, 341, 348, 351, 354, 357, 360], [454, 457, 460, 463, 466, 469, 472, 475, 478, 481], [486, 489, 492, 495, 498, 501, 504, 507, 510, 513], [593, 596, 599, 602, 605, 608, 611, 614, 617, 620], [795, 798, 801, 804, 807, 810, 813, 816, 819, 822], [364, 367, 370, 373, 376, 379, 382, 385, 388], [393, 396, 399, 402, 405, 408, 411, 414, 417], [424, 427, 430, 433, 436, 439, 442, 446, 449], [699, 702, 705, 708, 711, 714, 717, 720, 723], [560, 563, 566, 569, 572, 575, 578, 581], [519, 522, 525, 528, 531, 534], [538, 541, 544, 547, 550, 553], [774, 777, 780, 783, 786, 789], [757, 760, 763, 766, 769], [861, 864, 868, 871, 874], [660, 663, 666, 669], [733, 736, 739, 742], [827, 830, 833, 836], [842, 845, 848, 851], [909, 912, 915, 918], [924, 927, 930, 933], [626, 629, 632], [639, 642, 645], [682, 685, 688], [888, 891, 894], [650, 653], [749, 752], [900, 903], [587], [676], [693], [727], [857], [881]]\n",
      "-- region without decoding_barcode: [[321, 324, 327, 330, 333, 336, 339, 342, 349, 352, 355, 358], [285, 288, 291, 294, 297, 300, 303, 306, 309, 312, 315], [793, 796, 799, 802, 805, 808, 811, 814, 817, 820, 823], [422, 425, 428, 431, 434, 437, 440, 443, 447, 450], [594, 597, 600, 603, 606, 609, 612, 615, 618, 621], [394, 397, 400, 403, 406, 409, 412, 415, 418], [455, 458, 461, 464, 467, 470, 473, 476, 479], [487, 490, 493, 496, 499, 502, 505, 508, 511], [697, 700, 703, 706, 709, 712, 715, 718, 721], [365, 368, 371, 374, 377, 380, 383, 386], [561, 564, 567, 570, 573, 576, 579], [517, 520, 523, 526, 529, 532], [539, 542, 545, 548, 551, 554], [775, 778, 781, 784, 787], [862, 866, 869, 872, 875], [922, 925, 928, 931, 934], [637, 640, 643, 646], [661, 664, 667, 670], [734, 737, 740, 743], [758, 761, 764, 767], [828, 831, 834, 837], [886, 889, 892, 895], [627, 630, 633], [747, 750, 753], [843, 846, 849], [910, 913, 916], [585, 588], [651, 654], [674, 677], [683, 686], [901, 904], [728], [855], [882]]\n",
      "-- region without decoding_barcode: [[283, 286, 289, 292, 295, 298, 301, 304, 307, 310, 313, 316], [322, 325, 328, 331, 334, 337, 340, 343, 350, 353, 356, 359], [485, 488, 491, 494, 497, 500, 503, 506, 509, 512], [794, 797, 800, 803, 806, 809, 812, 815, 818, 821], [392, 395, 398, 401, 404, 407, 410, 413, 416], [423, 426, 429, 432, 435, 438, 441, 445, 448], [456, 459, 462, 465, 468, 471, 474, 477, 480], [595, 598, 601, 604, 607, 610, 613, 616, 619], [698, 701, 704, 707, 710, 713, 716, 719, 722], [366, 369, 372, 375, 378, 381, 384, 387], [559, 562, 565, 568, 571, 574, 577, 580], [518, 521, 524, 527, 530, 533], [773, 776, 779, 782, 785, 788], [540, 543, 546, 549, 552], [863, 867, 870, 873, 876], [659, 662, 665, 668], [732, 735, 738, 741], [759, 762, 765, 768], [841, 844, 847, 850], [887, 890, 893, 896], [908, 911, 914, 917], [923, 926, 929, 932], [625, 628, 631], [638, 641, 644], [829, 832, 835], [586, 589], [652, 655], [675, 678], [684, 687], [748, 751], [692], [856], [880], [902]]\n",
      "Number of clusters in each color:\n",
      "[1, 1, 1]\n",
      "- Assign barcodes\n",
      "-- Assigning decoding barcodes.\n",
      "--- Checking decoding barcodes.\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26]\n",
      "--- True\n",
      "-- Assigning TAD barcodes.\n",
      "-- Assigning unique barcodes.\n",
      "- Save to file: /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36/total_encoding.pkl\n"
     ]
    }
   ],
   "source": [
    "# NOTICE:\n",
    "# sub_reg_id_dic and sub_reg_size_dic are not used here because of failure in designing probes\n",
    "reg_encodings, hyb_matrix, assign_regs = Design_Encoding(reg_id_dic=new_id_dic, reg_size_dic=reg_len_dic, \n",
    "                                                         n_hyb=9, n_reg=36, filling_rows=False, \n",
    "                                                         save_folder=region_folder);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[281, 320, 451, 483, 590, 792, 361, 390, 421, 696, 556, 516, 535,\n",
       "         771, 754, 858, 657, 730, 824, 839, 906, 921, 623, 636, 679, 885,\n",
       "         647, 746, 897, 584, 673, 690, 724, 854, 878, 209]],\n",
       "\n",
       "       [[318, 282, 790, 419, 591, 391, 452, 484, 694, 362, 557, 514, 536,\n",
       "         772, 859, 919, 634, 658, 731, 755, 825, 883, 624, 744, 840, 907,\n",
       "         582, 648, 671, 680, 898, 725, 852, 879, 211, 691]],\n",
       "\n",
       "       [[266, 319, 482, 791, 389, 420, 453, 592, 695, 363, 555, 515, 770,\n",
       "         537, 860, 656, 729, 756, 838, 884, 905, 920, 622, 635, 826, 583,\n",
       "         649, 672, 681, 745, 689, 853, 877, 899, 213, 726]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_regs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Design sub library encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Sub_Library_Encoding(total_encoding, hyb_matrix, assign_regs, reg_id_dic,\n",
    "                         sub_library_size,\n",
    "                         min_reg_in_tad=2, \n",
    "                         save=True, save_folder=r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_7by21',\n",
    "                         continue_num=False,\n",
    "                         verbose=True):\n",
    "    '''Extract a sub library for total library and redesign encodings\n",
    "    Inputs:\n",
    "        _reg_encoding: region_number -> color=i, cluster=j, region=k, barcodes->...\n",
    "        hyb_matrix: hybridization matrix, n_reg by n_hyb\n",
    "        assign_regs: matrix of assigning region to clusters, n_color by n_cluster by n_reg\n",
    "        reg_id_dic: dictionary for region -> tad, dic\n",
    "        sub_library_size: number of regions in the sub library, int\n",
    "        min_reg_in_tad: criteria for selecting sub library, at least 2 regions in each new tad, int\n",
    "        save: whether save, bool\n",
    "        save_folder: directory for saving, str\n",
    "        continue_num: whether barcode id numbered continuously, False/'tad'/'decoding'/'all'\n",
    "        verbose: whether say something!, bool\n",
    "    Outputs:\n",
    "        sub_encodings: encoding scheme for sub library\n",
    "        other_encodings: encoding scheme for the rest of library\n",
    "    '''\n",
    "    # imports\n",
    "    import numpy as np;\n",
    "    \n",
    "    def _TAD_in_Cluster(_assign_regs, reg_id_dic=reg_id_dic, _verbose=verbose):\n",
    "        # input parameters\n",
    "        n_color = _assign_regs.shape[0]; # number of colors\n",
    "        n_cluster = _assign_regs.shape[1]; # number of clusters per color\n",
    "        n_reg = _assign_regs.shape[2]; # number of regions, defined by hyb matrix\n",
    "        \n",
    "        _assign_tads = -np.ones(np.shape(_assign_regs), dtype=np.int)\n",
    "        for _color in range(n_color):\n",
    "            for _cluster in range(n_cluster):\n",
    "                for _reg in range(n_reg):\n",
    "                    if _assign_regs[_color, _cluster, _reg] >= 0:\n",
    "                        _assign_tads[_color, _cluster, _reg] = reg_id_dic[_assign_regs[_color, _cluster, _reg]]\n",
    "        \n",
    "        return _assign_tads;\n",
    "    \n",
    "    def _Select_Sub_Encodings(total_encoding=total_encoding, assign_regs=assign_regs, \n",
    "                              sub_library_size=sub_library_size, min_reg_in_tad=min_reg_in_tad,\n",
    "                              _verbose=verbose):\n",
    "        from math import ceil\n",
    "        \n",
    "        if _verbose:\n",
    "            print \"-- Starting sub library searching\";\n",
    "        # convert assign_cluster into assign_tad\n",
    "        assign_tads = _TAD_in_Cluster(assign_regs);\n",
    "\n",
    "        # record parameters\n",
    "        n_color = assign_regs.shape[0]; # number of colors\n",
    "        n_cluster = assign_regs.shape[1]; # number of clusters per color\n",
    "        n_reg = assign_regs.shape[2]; # number of regions, defined by hyb matrix\n",
    "        _select_clusters = int(sub_library_size / n_reg) # number total selected clusters (in all colors)\n",
    "        if _verbose:\n",
    "            print \"--- color: \"+str(n_color), \"cluster: \"+str(n_cluster), \"region: \"+str(n_reg), \"selected clusters: \"+str(_select_clusters)\n",
    "        # Split select clusters in different colors equally\n",
    "        n_chooses = []\n",
    "        for i in range(n_color):\n",
    "            _choose =  int(ceil((_select_clusters-sum(n_chooses)) / float(n_color-i)));\n",
    "            n_chooses.append(_choose)\n",
    "        n_chooses = sorted(n_chooses)\n",
    "        n_chooses.reverse()\n",
    "        if _verbose:\n",
    "            print \"--- Choosing from each color:\", n_chooses;\n",
    "        # Randomly generate region picking\n",
    "        j=0\n",
    "        min_reg = -1;\n",
    "        while min_reg < min_reg_in_tad:\n",
    "            _cids = []; # chosen id list\n",
    "            _ctads = []; # chosen tad matrix parts\n",
    "            for i in range(n_color):\n",
    "                # chosen ids\n",
    "                _cids.append([sorted(np.random.choice(n_cluster, n_chooses[i], replace=False))])\n",
    "                # chosen tads\n",
    "                _ctads.append(assign_tads[i, _cids[i], :]);\n",
    "                # get unique set\n",
    "                _tads, _cts = np.unique(np.concatenate(_ctads,1), return_counts=True);\n",
    "            # check that no '-1' exist in this subset\n",
    "            missing_reg = False;\n",
    "            for _i in range(n_color):\n",
    "                for _id in _cids[_i]:\n",
    "                    if -1 in assign_regs[_i,_id,:]:\n",
    "                        missing_reg = True;\n",
    "            \n",
    "            # start updating once all TADs show up\n",
    "            if len(_tads) == len(np.unique(assign_tads[:,:-1,:])) and not missing_reg: \n",
    "                min_reg = np.min(_cts) # the minimum occurance of TADs\n",
    "            # if no threshold applied, directly update\n",
    "            elif min_reg_in_tad==0 and not missing_reg:\n",
    "                min_reg = np.min(_cts) # the minimum occurance of TADs\n",
    "            j+=1;\n",
    "        if _verbose:\n",
    "            print \"--- Number of searches:\", j;\n",
    "            print \"-- Finishing library searching, constructing sub library\";\n",
    "        \n",
    "        # Storing information into reg matrix\n",
    "        _sub_regs = -np.ones([n_color, n_chooses[0], n_reg]);\n",
    "        _other_regs = -np.ones([n_color, n_cluster-n_chooses[-1], n_reg]);\n",
    "        for _color in range(n_color):\n",
    "            _sub_regs[_color,:n_chooses[_color],:] = assign_regs[_color, _cids[_color],:] # sub region\n",
    "            _oid = list(set(np.arange(n_cluster)) - set(sorted(np.random.choice(22,5,replace=False)))) #other region\n",
    "            _other_regs[_color,:len(_oid),:] = assign_regs[_color, _oid, :];\n",
    "        \n",
    "        # Initialize encoding region list\n",
    "        _sub_encodings, _other_encodings = {},{};\n",
    "        for _r in np.unique(_sub_regs):\n",
    "            if _r >=0:\n",
    "                #_sub_encodings[int(_r)] = total_encoding[int(_r)];\n",
    "                _sub_encodings[int(_r)] = {'TAD':total_encoding[int(_r)]['TAD'],\n",
    "                                           'color':total_encoding[int(_r)]['color'],\n",
    "                                           'cluster':None,\n",
    "                                           'id':None,\n",
    "                                           'region':total_encoding[int(_r)]['region'],\n",
    "                                           'bc_decoding':None, 'bc_tad':None, 'bc_unique':None}\n",
    "        for _r in np.unique(_other_regs):\n",
    "            if _r >=0:\n",
    "                #_other_encodings[int(_r)] = total_encoding[int(_r)];   \n",
    "                _other_encodings[int(_r)] = {'TAD':total_encoding[int(_r)]['TAD'],\n",
    "                                             'color':total_encoding[int(_r)]['color'],\n",
    "                                             'cluster':None,\n",
    "                                             'id':None,\n",
    "                                             'region':total_encoding[int(_r)]['region'],\n",
    "                                             'bc_decoding':None, 'bc_tad':None, 'bc_unique':None}\n",
    "        print _sub_regs;\n",
    "        return _sub_encodings, _sub_regs, _other_encodings, _other_regs\n",
    "    \n",
    "    \n",
    "    def _Assign_All_Barcodes(_reg_encodings, _assign_regs, _hyb_matrix=hyb_matrix, \n",
    "                             _continue_num=continue_num, _verbose=verbose):\n",
    "        '''Assembled function to update all barcodes'''\n",
    "        # record parameters\n",
    "        n_color = _assign_regs.shape[0]; # number of colors\n",
    "        n_cluster = _assign_regs.shape[1]; # number of clusters per color\n",
    "        n_reg = _assign_regs.shape[2]; # number of regions per cluster, defined by hyb matrix\n",
    "        n_hyb = _hyb_matrix.shape[1]; # number of hybes per cluster\n",
    "        if _verbose:\n",
    "            print \"--- color: \"+str(n_color), \"cluster: \"+str(n_cluster), \"region: \"+str(n_reg),\\\n",
    "                \"hybs: \"+str(n_hyb);\n",
    "        def _Assign_Decoding_Barcodes(_reg_encodings, _assign_regs=_assign_regs, _hyb_matrix=_hyb_matrix,\n",
    "                                      n_color=n_color, n_cluster=n_cluster, \n",
    "                                      n_reg=n_reg, n_hyb=n_hyb, _verbose=verbose):\n",
    "            '''Assign barcode (orders) used for decoding'''\n",
    "            if _verbose:\n",
    "                print '-- Assigning decoding barcodes.'        \n",
    "            # Sanity check\n",
    "            if np.shape(_assign_regs)[0] != n_color or np.shape(_assign_regs)[2] != n_reg:\n",
    "                raise EOFError('wrong input dimension!');\n",
    "            # collect number of clusters per color\n",
    "            _barcode_set = 0; # barcode to be assigned\n",
    "            for _color in range(n_color):\n",
    "                for _cluster in range(n_cluster):\n",
    "                    for _reg in range(n_reg):\n",
    "                        if _assign_regs[_color,_cluster,_reg] >= 0:\n",
    "                            _reg_encodings[_assign_regs[_color,_cluster,_reg]]['cluster'] = _cluster\n",
    "                            _reg_encodings[_assign_regs[_color,_cluster,_reg]]['bc_decoding'] = [n_hyb*_barcode_set+i for i, j in enumerate(_hyb_matrix[_reg]) if j == 1]\n",
    "                            #print [n_hyb*_barcode_set+i for i, j in enumerate(_hyb_matrix[_reg]) if j == 1]\n",
    "                    _barcode_set += 1; # next barcode set (size of n_hyb)\n",
    "            return _reg_encodings;\n",
    "\n",
    "        def _Assign_TAD_Barcodes(_reg_encodings, _continue_num=_continue_num, _verbose=verbose):\n",
    "            '''Assign barcode (orders) used for TAD identity'''\n",
    "            if _verbose:\n",
    "                print '-- Assigning TAD barcodes.' \n",
    "            # record all decoding barcodes\n",
    "            dec_bcs = []\n",
    "            for k,v in _reg_encodings.iteritems():\n",
    "                dec_bcs += v['bc_decoding']\n",
    "            # tad barcodes should start right after\n",
    "            if _continue_num == 'all':\n",
    "                tad_bc_start = max(dec_bcs)+1; \n",
    "            else:\n",
    "                tad_bc_start = 0;\n",
    "            for k,v in _reg_encodings.iteritems():\n",
    "                if v['TAD']>=0:\n",
    "                    _reg_encodings[k]['bc_tad'] = _reg_encodings[k]['TAD'] + tad_bc_start;\n",
    "\n",
    "            return _reg_encodings\n",
    "\n",
    "        def _Assign_Unique_Barcodes(_reg_encodings, _continue_num=_continue_num, _verbose=verbose):\n",
    "            '''Assign barcode (orders) used for unique sequential'''\n",
    "            if _verbose:\n",
    "                print '-- Assigning unique barcodes.'\n",
    "\n",
    "            # unique barcodes should start right after\n",
    "            if _continue_num == 'tad':\n",
    "                # record decoding TAD barcodes\n",
    "                used_bcs = []\n",
    "                for k,v in _reg_encodings.iteritems():\n",
    "                    used_bcs += [v['bc_tad']]\n",
    "                unique_bc_start = max(used_bcs)+1; \n",
    "            elif _continue_num == 'decoding':\n",
    "                # record decoding barcodes barcodes\n",
    "                used_bcs = []\n",
    "                for k,v in _reg_encodings.iteritems():\n",
    "                    used_bcs += v['bc_decoding']\n",
    "                unique_bc_start = max(used_bcs)+1; \n",
    "            elif  _continue_num == 'all':              \n",
    "                # record all decoding barcodes and TAD barcodes\n",
    "                used_bcs = []\n",
    "                for k,v in _reg_encodings.iteritems():\n",
    "                    used_bcs += v['bc_decoding']\n",
    "                    used_bcs += [v['bc_tad']]\n",
    "                unique_bc_start = max(used_bcs)+1; \n",
    "            else:\n",
    "                unique_bc_start = 0\n",
    "                \n",
    "            reg_new_id = 0;\n",
    "            for k,v in sorted(_reg_encodings.items()):\n",
    "                _reg_encodings[k]['bc_unique'] = reg_new_id + unique_bc_start;\n",
    "                _reg_encodings[k]['id'] = reg_new_id;\n",
    "                reg_new_id += 1;\n",
    "\n",
    "            return _reg_encodings  \n",
    "        \n",
    "        # assign decoding barcodes\n",
    "        _reg_encodings = _Assign_Decoding_Barcodes(_reg_encodings)\n",
    "        # assign TAD barcodes\n",
    "        _reg_encodings = _Assign_TAD_Barcodes(_reg_encodings, _continue_num=_continue_num)\n",
    "        # assign unique barcodes\n",
    "        _reg_encodings = _Assign_Unique_Barcodes(_reg_encodings, _continue_num=_continue_num)    \n",
    "    \n",
    "        return _reg_encodings\n",
    "    \n",
    "    \n",
    "    # Select sub library\n",
    "    if verbose:\n",
    "        print \"- Select sub library.\"\n",
    "    sub_encodings, sub_regs, other_encodings, other_regs= _Select_Sub_Encodings()\n",
    "    # Re_assign barcodes\n",
    "    if verbose:\n",
    "        print \"- Reassign barcodes for sub library.\"\n",
    "        print \"-- continue numbering:\", continue_num;\n",
    "    sub_encodings = _Assign_All_Barcodes(sub_encodings, sub_regs);\n",
    "    if verbose:\n",
    "        print \"- Reassign barcodes for the rest of library.\"\n",
    "        print \"-- continue numbering:\", continue_num;\n",
    "    other_encodings = _Assign_All_Barcodes(other_encodings, other_regs);    \n",
    "    \n",
    "    if save:\n",
    "        import cPickle as pickle\n",
    "        import os\n",
    "        sub_filename = save_folder + os.sep + 'sub_encoding.pkl';\n",
    "        other_filename = save_folder + os.sep + 'other_encoding.pkl';\n",
    "        if verbose:\n",
    "            print \"- Save to file:\", sub_filename, other_filename\n",
    "        # save\n",
    "        pickle.dump(sub_encodings, open(sub_filename,'w'))\n",
    "        pickle.dump(other_encodings, open(other_filename,'w'))\n",
    "    \n",
    "    return sub_encodings, other_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36\n",
      "- Select sub library.\n",
      "-- Starting sub library searching\n",
      "--- color: 3 cluster: 1 region: 36 selected clusters: 2\n",
      "--- Choosing from each color: [1, 1, 0]\n",
      "--- Number of searches: 1\n",
      "-- Finishing library searching, constructing sub library\n",
      "[[[ 281.  320.  451.  483.  590.  792.  361.  390.  421.  696.  556.  516.\n",
      "    535.  771.  754.  858.  657.  730.  824.  839.  906.  921.  623.  636.\n",
      "    679.  885.  647.  746.  897.  584.  673.  690.  724.  854.  878.  209.]]\n",
      "\n",
      " [[ 318.  282.  790.  419.  591.  391.  452.  484.  694.  362.  557.  514.\n",
      "    536.  772.  859.  919.  634.  658.  731.  755.  825.  883.  624.  744.\n",
      "    840.  907.  582.  648.  671.  680.  898.  725.  852.  879.  211.  691.]]\n",
      "\n",
      " [[  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "     -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "     -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]]]\n",
      "- Reassign barcodes for sub library.\n",
      "-- continue numbering: tad\n",
      "--- color: 3 cluster: 1 region: 36 hybs: 9\n",
      "-- Assigning decoding barcodes.\n",
      "-- Assigning TAD barcodes.\n",
      "-- Assigning unique barcodes.\n",
      "- Reassign barcodes for the rest of library.\n",
      "-- continue numbering: tad\n",
      "--- color: 3 cluster: 1 region: 36 hybs: 9\n",
      "-- Assigning decoding barcodes.\n",
      "-- Assigning TAD barcodes.\n",
      "-- Assigning unique barcodes.\n",
      "- Save to file: /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36/sub_encoding.pkl /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36/other_encoding.pkl\n"
     ]
    }
   ],
   "source": [
    "print region_folder\n",
    "sub_encodings, other_encodings = Sub_Library_Encoding(reg_encodings, hyb_matrix, assign_regs, new_id_dic, 72, \n",
    "                                                      min_reg_in_tad=0,\n",
    "                                                      continue_num='tad',\n",
    "                                                      save_folder=region_folder);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_encodings.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Patch Barcode Sequence to Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minimal imports for biopython\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import os,glob,time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Read barcode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barcodes loaded: Stv: 27, NDB: 1052\n"
     ]
    }
   ],
   "source": [
    "# read all Stv barcodes\n",
    "barcode_dir = r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Barcodes';\n",
    "\n",
    "#stv_adaptor = [1,2,17,62,77,78,79,80,81,82,83,84] # barcodes saved for adaptors\n",
    "#stv_bad = [34,38,41] # barcodes performed badly\n",
    "#stv_mask = stv_adaptor + stv_bad \n",
    "stv_mask = []\n",
    "\n",
    "with open(barcode_dir+os.sep+'top_Stvs_select27.fasta', \"rU\") as handle:\n",
    "    stv_barcodes = [];\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        if int(record.id.split('_')[1]) not in stv_mask:\n",
    "            stv_barcodes.append(record);\n",
    "\n",
    "# read all NDB barcodes\n",
    "ndb_mask = [];\n",
    "\n",
    "with open(barcode_dir+os.sep+'NDBs.fasta', \"rU\") as handle:\n",
    "    ndb_barcodes = [];\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        if int(record.id.split('_')[1]) not in ndb_mask:\n",
    "            ndb_barcodes.append(record);\n",
    "print \"Barcodes loaded: Stv: \"+str(len(stv_barcodes))+\", NDB: \"+str(len(ndb_barcodes));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Read all PCR primers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primers loaded: forward: 12, reverse: 9\n"
     ]
    }
   ],
   "source": [
    "primer_dir = r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Primers';\n",
    "fwd_primer_filename = 'forward_primers_keep.fasta';\n",
    "rev_primer_filename = 'reverse_primers_keep.fasta';\n",
    "\n",
    "# read all forward primers\n",
    "with open(primer_dir+os.sep+fwd_primer_filename, \"rU\") as handle:\n",
    "    fwd_primers = [];\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        fwd_primers.append(record);\n",
    "# read all forward primers\n",
    "with open(primer_dir+os.sep+rev_primer_filename, \"rU\") as handle:\n",
    "    rev_primers = [];\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        rev_primers.append(record);\n",
    "print \"Primers loaded: forward: \"+str(len(fwd_primers))+\", reverse: \"+str(len(rev_primers));        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 read all probe reports and generate primary probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master_directory:\n",
      "/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36\n",
      "Saving_directory:\n",
      "/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36/final_probes\n",
      "- forward primer: ID: W1A07_primer_6\n",
      "Name: W1A07_primer_6\n",
      "Description: W1A07_primer_6\n",
      "Number of features: 0\n",
      "Seq('CGCAAACTGGTGCGGAAGGC', SingleLetterAlphabet())\n",
      "- reverse primer: ID: W1A12_primer_11\n",
      "Name: W1A12_primer_11\n",
      "Description: W1A12_primer_11\n",
      "Number of features: 0\n",
      "Seq('TAATACGACTCACTATAGGGCCATTGCCCGCGAGGTCGAG', SingleLetterAlphabet())\n"
     ]
    }
   ],
   "source": [
    "# Important inputs for patching barcodes\n",
    "barcode_source = {'bc_unique':'ndb',\n",
    "                  'bc_decoding':'stv'};\n",
    "barcode_order = ['bc_decoding', 'bc_unique'];\n",
    "\n",
    "# master directory\n",
    "master_dir =r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36';\n",
    "report_folder = r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400'; # if merged\n",
    "save_folder = r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36/final_probes'; # if merged\n",
    "print \"Master_directory:\\n\", master_dir;\n",
    "print \"Saving_directory:\\n\", save_folder;\n",
    "\n",
    "# primer sets\n",
    "fprimer = fwd_primers[3];\n",
    "print '- forward primer:', fprimer\n",
    "rprimer = rev_primers[5];\n",
    "print '- reverse primer:', rprimer\n",
    "\n",
    "# dic for sub-encoding scheme\n",
    "if not 'sub_encodings' in vars():\n",
    "    import cPickle as pickle\n",
    "    print 'loading sub_encodings'\n",
    "    sub_encodings = pickle.load(open(master_dir+os.sep+'sub_encoding.pkl','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Patch_Barcodes(reg_encodings, \n",
    "                   fwd_primer,rev_primer,\n",
    "                   barcode_source, \n",
    "                   barcode_order,\n",
    "                   stv_barcodes, ndb_barcodes, barcode_starts={'stv':1,'ndb':1},\n",
    "                   report_folder=r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged',\n",
    "                   save_folder=r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/final_probes',\n",
    "                   add_rand_gap=0,\n",
    "                   save=True, verbose=True):\n",
    "    '''Function to patch barcodes to designed probes\n",
    "    Inputs:\n",
    "        reg_encodings: encoding scheme for the barcode, dictionary(generated previously)\n",
    "        fwd_primer: forward primer,20mer, biopython SeqRecord\n",
    "        rev_primer: reverse primer,40mer(rc), last 20mer-rc should be used\n",
    "        barcode_source: dictionary to determine the source of barcodes, dictionary\n",
    "        barcode_order: list to determine numbering order of barcodes, list\n",
    "        stv_barcodes: old barcodes,30mer, biopython SeqRecord list\n",
    "        ndb_barcodes: new barcodes,30mer, biopython SeqRecord list\n",
    "        barcode_starts: id of the first unused barcode, dictionary\n",
    "        report_folder: directory for probe reports, string\n",
    "        save_folder: directory for save files, string\n",
    "        add_rand_gap: whether adding (or length) of random gaps between barcodes, int\n",
    "        save: whether save, bool\n",
    "        verbose: whether say something, bool\n",
    "    Outputs:\n",
    "        total library SeqRecord\n",
    "        '''\n",
    "    # minimal imports\n",
    "    from Bio import SeqIO\n",
    "    from Bio.Seq import Seq\n",
    "    from Bio.Alphabet import IUPAC\n",
    "    from Bio.SeqRecord import SeqRecord \n",
    "    import numpy as np;\n",
    "    import glob, os, sys, time\n",
    "    import LibraryDesigner as ld\n",
    "    \n",
    "    # check inputs:\n",
    "    if verbose:\n",
    "        print \"- Check inputs\"\n",
    "    # check barcode_source\n",
    "    barcode_types = reg_encodings.values()[0].keys();\n",
    "    for k, v in barcode_source.iteritems():\n",
    "        if k not in barcode_types:\n",
    "            raise ValueError('wrong barcode_source input!');\n",
    "    # check barcode_order\n",
    "    for _name in barcode_order:\n",
    "        if _name not in barcode_types:\n",
    "            raise ValueError('wrong barcode_order input!');\n",
    "            \n",
    "    # filter stv_barcodes and ndb_barcodes\n",
    "    if verbose:\n",
    "        print \"- check barcode starts: \", barcode_starts\n",
    "    _stv_barcodes, _ndb_barcodes = [],[];\n",
    "    for record in stv_barcodes:\n",
    "        if not int(record.id.split('_')[1]) < barcode_starts['stv']:\n",
    "            _stv_barcodes.append(record)\n",
    "    for record in ndb_barcodes:\n",
    "        if not int(record.id.split('_')[1]) < barcode_starts['ndb']:\n",
    "            _ndb_barcodes.append(record)\n",
    "    \n",
    "    def _generating_file_encoding(_report_folder=report_folder, \n",
    "                                  _reg_encodings=reg_encodings, _verbose=verbose):\n",
    "        '''Convert region id encoding scheme into filename encoding scheme, change keys\n",
    "        Inputs: \n",
    "            report_folder\n",
    "            reg_encodings\n",
    "            verbose\n",
    "        Output:\n",
    "            pb_files\n",
    "            file_encodings'''\n",
    "        # load probe reports:\n",
    "        _pb_files = [fl for fl in glob.glob(_report_folder+os.sep+r'*.pbr') if int(os.path.basename(fl).split('_')[1].split('.')[0]) in _reg_encodings.keys()]\n",
    "        if _verbose:\n",
    "            print \"- Load probe reports, total_num:\", len(_pb_files);\n",
    "        # save to file_encodings\n",
    "        _file_encodings = {};\n",
    "        for fl in _pb_files:\n",
    "            _file_encodings[fl] = _reg_encodings[int(os.path.basename(fl).split('_')[1].split('.')[0])];\n",
    "        \n",
    "        return _pb_files, _file_encodings;\n",
    "    \n",
    "\n",
    "    \n",
    "    def _patch_barcode_per_file(_file, _file_encodings, \n",
    "                                _fwd_primer=fwd_primer, _rev_primer=rev_primer,\n",
    "                                _barcode_source=barcode_source, _stv_barcodes=stv_barcodes, _ndb_barcodes=ndb_barcodes,\n",
    "                                _add_rand_gap=add_rand_gap, _verbose=verbose):\n",
    "        from random import choice\n",
    "        import os\n",
    "        if _verbose:\n",
    "            print \"-- patch barcodes :\", _file\n",
    "        # load probe report\n",
    "        _pb = ld.pb_reports_class()\n",
    "        _pb.load_pbr(_file)\n",
    "        \n",
    "        # extract encoding info:\n",
    "        _encoding = _file_encodings[_file];\n",
    "        \n",
    "        # initialize, save all infos here\n",
    "        _plist = [];\n",
    "        _precords = [];\n",
    "        for _info in _pb.pb_reports_keep.values():\n",
    "            _tmp_info = _info.copy();\n",
    "\n",
    "            # extract all encoding info from reg_encodings\n",
    "            _tmp_info['reg_index'] = _encoding['id']\n",
    "            _tmp_info['color'] = _encoding['color']\n",
    "            if 'gene' in _encoding.keys():\n",
    "                _tmp_info['gene'] = _encoding['gene']\n",
    "\n",
    "            # extract barcode info\n",
    "            _islist = False; # variable used for later design\n",
    "            for _k,_v in _barcode_source.iteritems():\n",
    "                if isinstance(_encoding[_k], list):\n",
    "                    _islist = _k; # variable used for later design\n",
    "                    _bcs = [];\n",
    "                    for _bid in _encoding[_k]:\n",
    "                        if _v == 'stv':\n",
    "                            _bcs.append(_stv_barcodes[_bid]);\n",
    "                        elif _v == 'ndb':\n",
    "                            _bcs.append(_ndb_barcodes[_bid]);\n",
    "                    _tmp_info[_k] = _bcs;\n",
    "                else:\n",
    "                    if _v == 'stv':\n",
    "                        _tmp_info[_k] =_stv_barcodes[_encoding[_k]];\n",
    "                    elif _v == 'ndb':\n",
    "                        _tmp_info[_k] =_ndb_barcodes[_encoding[_k]];\n",
    "            # extract primer info:\n",
    "            _tmp_info['fwd_primer'] = _fwd_primer;\n",
    "            _tmp_info['rev_primer'] = _rev_primer;\n",
    "\n",
    "            ## generate_whole sequence\n",
    "            # fwd_primer(20)\n",
    "            # barcode 1 [from list, 1], (reverse-complement of last 20)\n",
    "            # barcode 2, (reverse-complement of last 20)\n",
    "            # target sequence\n",
    "            # barcode 3, (reverse-complement of last 20)\n",
    "            # barcode 4 [from list, 1], (reverse-complement of last 20)\n",
    "            # rev_primer, (reverse-complement of last 20)\n",
    "            _seq_list = []; # start\n",
    "            _seq_list.append(_tmp_info['fwd_primer'].seq) # fwd primer\n",
    "            if _islist:\n",
    "                _seq_list += [_bc.seq[-20:].reverse_complement() for _bc in _tmp_info[_islist]]; # list barcodes, usually for decoding\n",
    "                for _k,_v in _barcode_source.iteritems():\n",
    "                    if _k != _islist:\n",
    "                        _seq_list.insert(-1, _tmp_info[_k].seq[-20:].reverse_complement()) # other barcodes\n",
    "                _seq_list.insert(-2, Seq(_tmp_info['seq']) ) # target sequence in the middle\n",
    "            else:\n",
    "                for _k,_v in _barcode_source.iteritems():\n",
    "                    _seq_list.append(_tmp_info[_k].seq[-20:].reverse_complement()) # other barcodes\n",
    "                _seq_list.insert(-2, Seq(_tmp_info['seq']) ) # target sequence in the middle\n",
    "\n",
    "            _seq_list.append(_tmp_info['rev_primer'].seq[-20:].reverse_complement()) # reverse primer\n",
    "            # result\n",
    "            dna_alphabet = ['A','A','C','G','T','T']; # used for adding random gap, if needed\n",
    "            _total_seq = Seq('');\n",
    "            for j in range(len(_seq_list)):\n",
    "                _seq = _seq_list[j]\n",
    "                _total_seq += _seq;\n",
    "                if j > 0 and j < len(_seq_list)-2:\n",
    "                    _total_seq += ''.join([choice(dna_alphabet) for i in range(_add_rand_gap)]);\n",
    "            _tmp_info['total_seq'] = _total_seq;\n",
    "\n",
    "            ## Generate total_name:\n",
    "            # chr21:10350001-10400001_reg_208_gene_chr21_pb_41577 (from base name)\n",
    "            # primer_[4,11]\n",
    "            # barcodes_75,109,[]\n",
    "\n",
    "            # base name\n",
    "            _total_name = _tmp_info['name'].split('reg_')[0] + 'reg_'+str(_tmp_info['reg_index']);\n",
    "            if 'gene' in _tmp_info['name']:\n",
    "                _total_name += '_gene' + _tmp_info['name'].split('gene')[1]\n",
    "            elif 'gene' in _tmp_info.keys():\n",
    "                _total_name += '_gene_'+_tmp_info['gene'];\n",
    "            # primer name\n",
    "            _primer_sets = [int(_tmp_info['fwd_primer'].id.split('_')[-1]), int(_tmp_info['rev_primer'].id.split('_')[-1])]\n",
    "            _total_name += '_primer_'+str(_primer_sets).replace(' ','')\n",
    "            # barcode name\n",
    "            _barcode_sets = [];\n",
    "            if _islist:\n",
    "                _barcode_sets.append([rec.id for rec in _tmp_info[_islist]]);\n",
    "                for _k,_v in _barcode_source.iteritems():\n",
    "                    if _k != _islist:\n",
    "                        _barcode_sets.append(_tmp_info[_k].id);\n",
    "            else:\n",
    "                for _k,_v in _barcode_source.iteritems():\n",
    "                    _barcode_sets.append(_tmp_info[_k].id);        \n",
    "            _total_name += '_barcodes_'+str(_barcode_sets).replace(' ','')\n",
    "            # color\n",
    "            _total_name += '_color_'+str(_tmp_info['color'])\n",
    "            \n",
    "            ## save\n",
    "            _tmp_info['total_name'] = _total_name;\n",
    "            ## Append\n",
    "            _plist.append(_tmp_info) # to plist\n",
    "            _precords.append(SeqRecord(_total_seq, id=_total_name, description='', name=_total_name)); # to seq record\n",
    "\n",
    "        return _plist, _precords    \n",
    "    \n",
    "    # generate file encoding\n",
    "    _pb_files, _file_encodings = _generating_file_encoding();\n",
    "\n",
    "    # initialize\n",
    "    _pb_lists, _pb_records = [],[];\n",
    "    # loop through all files\n",
    "    for _fl in sorted(_pb_files, key=lambda fl:int(fl.split('_')[-1].split('.')[0])):\n",
    "        _list, _records = _patch_barcode_per_file(_fl, _file_encodings);\n",
    "        _pb_lists.append(_list);\n",
    "        _pb_records += _records\n",
    "    \n",
    "    # save:\n",
    "    if save:\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        list_savefile = save_folder + os.sep + 'list.pkl';\n",
    "        pb_savefile = save_folder + os.sep + 'candidate_probes.fasta';\n",
    "        if verbose:\n",
    "            print \"- Saving list to:\", list_savefile\n",
    "        pickle.dump(_pb_lists, open(list_savefile,'w'));\n",
    "        if verbose:\n",
    "            print \"- Saving probes to:\", pb_savefile\n",
    "        with open(pb_savefile, 'w') as output_handle:\n",
    "            SeqIO.write(_pb_records, output_handle, 'fasta');\n",
    "        \n",
    "    return _pb_lists, _pb_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Check inputs\n",
      "- check barcode starts:  {'stv': 1, 'ndb': 1}\n",
      "- Load probe reports, total_num: 72\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_209.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_211.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_281.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_282.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_318.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_320.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_361.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_362.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_390.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_391.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_419.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_421.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_451.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_452.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_483.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_484.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_514.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_516.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_535.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_536.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_556.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_557.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_582.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_584.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_590.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_591.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_623.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_624.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_634.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_636.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_647.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_648.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_657.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_658.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_671.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_673.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_679.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_680.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_690.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_691.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_694.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_696.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_724.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_725.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_730.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_731.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_744.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_746.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_754.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_755.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_771.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_772.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_790.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_792.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_824.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_825.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_839.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_840.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_852.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_854.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_858.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_859.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_878.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_879.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_883.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_885.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_897.pbr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_898.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_906.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_907.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_919.pbr\n",
      "-- patch barcodes : /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21/reports/centered_merged-400/reg_921.pbr\n",
      "- Saving list to: /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36/final_probes/list.pkl\n",
      "- Saving probes to: /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36/final_probes/candidate_probes.fasta\n"
     ]
    }
   ],
   "source": [
    "pb_lists, pb_records = Patch_Barcodes(reg_encodings=sub_encodings,\n",
    "                                      fwd_primer=fprimer, rev_primer=rprimer, \n",
    "                                      barcode_source=barcode_source, barcode_order=barcode_order, \n",
    "                                      stv_barcodes=stv_barcodes, ndb_barcodes=ndb_barcodes, \n",
    "                                      report_folder=report_folder, save_folder=save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36\n",
      "- forward primer: ID: W1A07_primer_6\n",
      "Name: W1A07_primer_6\n",
      "Description: W1A07_primer_6\n",
      "Number of features: 0\n",
      "Seq('CGCAAACTGGTGCGGAAGGC', SingleLetterAlphabet())\n",
      "- reverse primer: ID: W1A12_primer_11\n",
      "Name: W1A12_primer_11\n",
      "Description: W1A12_primer_11\n",
      "Number of features: 0\n",
      "Seq('TAATACGACTCACTATAGGGCCATTGCCCGCGAGGTCGAG', SingleLetterAlphabet())\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "barcode_source = {'bc_decoding':'stv',\n",
    "                  'bc_unique':'ndb'};\n",
    "master_dir =r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36';\n",
    "pb_dir = r'final_probes';\n",
    "print master_dir\n",
    "# primers\n",
    "fprimer = fwd_primers[3];\n",
    "print '- forward primer:', fprimer\n",
    "rprimer = rev_primers[5];\n",
    "print '- reverse primer:', rprimer\n",
    "\n",
    "# dic for region -> tad\n",
    "if not 'sub_encodings' in vars():\n",
    "    print 'loading sub_encodings'\n",
    "    sub_encodings = pickle.load(open(master_dir+os.sep+'sub_encoding.pkl','r'))\n",
    "if not 'pb_records' in vars():\n",
    "    print '- loading all probes'\n",
    "    with open(master_dir+os.sep+pb_dir+os.sep+'candidate_probes.fasta', \"rU\") as handle:\n",
    "        pb_records = [];\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            pb_records.append(record);\n",
    "if not 'pb_lists' in vars():\n",
    "    print '- loading pb_lists'\n",
    "    pb_lists = pickle.load(open(master_dir+os.sep+pb_dir+os.sep+'list.pkl', \"rU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Check_Probes(pb_records, pb_lists, reg_encodings, master_dir, \n",
    "                 fwd_primer,rev_primer,\n",
    "                 stv_barcodes, ndb_barcodes, barcode_starts={'stv':1,'ndb':1},\n",
    "                 report_dir=r'reports/centered_merged',save_dir=r'final_probes',\n",
    "                 add_rand_gap=0, total_bc=4, barcode_len=20, target_len=42,  \n",
    "                 word_size=17, max_internal_hits=5, max_genome_hits=150,\n",
    "                 index_dir=r'/n/boslfs/LABS/zhuang_lab/User/pzheng/Indeces/human/hg38',\n",
    "                 save=True, verbose=True):\n",
    "    # imports\n",
    "    import os,glob,sys\n",
    "    sys.path.append(r'/n/home13/pzheng/Documents/python-functions/python-functions-library')\n",
    "    from LibraryConstruction import fastaread,fastawrite,fastacombine\n",
    "    import LibraryDesigner as ld\n",
    "    import numpy as np\n",
    "    \n",
    "    def _check_primer_usage(pb_records=pb_records, fwd_primer=fwd_primer, rev_primer=rev_primer,\n",
    "                            _verbose=verbose):\n",
    "        '''Check whether forward or reverse primer are used in all probes'''\n",
    "        if _verbose:\n",
    "            print \"-- Checking primer usage, total probes:\", len(pb_records)\n",
    "        fwd_len = len(fwd_primer.seq);\n",
    "        rev_len = len(rev_primer.seq[-20:].reverse_complement());\n",
    "        \n",
    "        for record in pb_records:\n",
    "            if record.seq[:fwd_len] != fwd_primer.seq:\n",
    "                if _verbose:\n",
    "                    print \"--- Forward primer incorrect!\"\n",
    "                return False\n",
    "            if record.seq[-rev_len:] != rev_primer.seq[-20:].reverse_complement():\n",
    "                if _verbose:\n",
    "                    print \"--- Forward primer incorrect!\"\n",
    "                return False\n",
    "        return True # if no error applies\n",
    "    \n",
    "    def _check_region_size(pb_records=pb_records, pb_lists=pb_lists):\n",
    "        '''Generate a dirctionary '''\n",
    "        # get original region size\n",
    "        _reg_size_dic = {}\n",
    "        for lst in pb_lists:\n",
    "            _reg_size_dic[lst[0]['reg_index']] = len(lst);\n",
    "        # get region size from probe names\n",
    "        _size_from_rec = {}\n",
    "        for record in pb_records:\n",
    "            reg_id = int(record.id.split('_reg_')[1].split('_')[0]);\n",
    "            if reg_id not in _size_from_rec.keys():\n",
    "                _size_from_rec[reg_id] = 1; # if not in key, create\n",
    "            else:\n",
    "                _size_from_rec[reg_id] += 1; # otherwise, add count\n",
    "        # compare\n",
    "        _match = True;\n",
    "        for k,v in sorted(_size_from_rec.items()):\n",
    "            if k not in _reg_size_dic.keys():\n",
    "                print \"region list and region id in probes not match for\", k\n",
    "                _match = False\n",
    "                break\n",
    "            else:\n",
    "                if v != _reg_size_dic[k]:\n",
    "                    print \"region size doesn't match for:\", k\n",
    "                    _match = False\n",
    "                    break\n",
    "        \n",
    "        return _reg_size_dic, _match;\n",
    "    \n",
    "    def _check_gene_size():\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def _check_region_to_barcode(pb_records=pb_records, stv_barcodes=stv_barcodes, ndb_barcodes=ndb_barcodes,\n",
    "                                 total_bc=total_bc):\n",
    "        '''Generate map from region id to barcodes used in this region'''\n",
    "        import re\n",
    "        _reg_to_barcode = {}\n",
    "        for record in pb_records:\n",
    "            # region id\n",
    "            reg_id = int(record.id.split('_reg_')[1].split('_')[0]);\n",
    "            if reg_id not in _reg_to_barcode.keys():\n",
    "                # barcode ids\n",
    "                stv_matches = re.findall('\\'Stv_(.+?)\\'', record.id, re.DOTALL)\n",
    "                ndb_matches = re.findall('\\'NDB_(.+?)\\'', record.id, re.DOTALL)\n",
    "                stv_names = ['Stv_'+str(stv_id) for stv_id in stv_matches]\n",
    "                ndb_names = ['NDB_'+str(ndb_id) for ndb_id in ndb_matches]\n",
    "                _reg_to_barcode[reg_id] = stv_names+ndb_names\n",
    "        \n",
    "        ## barcode check\n",
    "        _barcode_check = True;\n",
    "        # barcode names\n",
    "        bc_names = [stv.id for stv in stv_barcodes] + [ndb.id for ndb in ndb_barcodes]\n",
    "        # search through previous dictionary\n",
    "        for reg,bcs in sorted(_reg_to_barcode.items()):\n",
    "            for bc in bcs:\n",
    "                if len(bcs) != total_bc:\n",
    "                    print \"-- Error in barcode number for region:\", reg\n",
    "                    _barcode_check = False\n",
    "                    break\n",
    "                if bc not in bc_names:\n",
    "                    print \"-- Wrong barcode name for barcode: \"+str(bc)+\", region: \"+str(reg)\n",
    "                    _barcode_check = False\n",
    "                    break\n",
    "        \n",
    "        return _reg_to_barcode, _barcode_check;\n",
    "        \n",
    "    def _parsing_probe_sequence(record, fwd_primer=fwd_primer, rev_primer=rev_primer,\n",
    "                                add_rand_gap=add_rand_gap, barcode_len=barcode_len, target_len=target_len):\n",
    "        '''parse a probe sequence to acquire all barcode binding sites'''\n",
    "        # take in a seq record, parse the sequence and return a list of all included barcodes (20mer,RC)\n",
    "        barcode_list = [];\n",
    "        _main_seq = record.seq[len(fwd_primer.seq):-20];\n",
    "        \n",
    "        \n",
    "        # trim last 2 barcodes\n",
    "        for i in range(2):\n",
    "            barcode_list.append(_main_seq[-barcode_len:]);\n",
    "            _main_seq = _main_seq[:-(barcode_len+add_rand_gap)];\n",
    "        # trim all barcodes from the beginning\n",
    "        while len(_main_seq) > target_len:\n",
    "            barcode_list.append(_main_seq[:barcode_len]);\n",
    "            _main_seq = _main_seq[(barcode_len+add_rand_gap):];\n",
    "        \n",
    "        return barcode_list;\n",
    "    \n",
    "    def _finding_barcode_name(barcode_list, stv_barcodes=stv_barcodes, ndb_barcodes=ndb_barcodes, \n",
    "                              barcode_len=barcode_len, total_bc=total_bc):\n",
    "        '''Given barcode list generated by parsing probe, return a list of barcode names'''\n",
    "        _name_list = [];\n",
    "        for bc_site in barcode_list:\n",
    "            for bc in stv_barcodes+ndb_barcodes:\n",
    "                if bc.seq[-barcode_len:] == bc_site.reverse_complement():\n",
    "                    _name_list.append(bc.id);\n",
    "                    break;\n",
    "        \n",
    "        if len(_name_list) < total_bc:\n",
    "            print \"-- Failed in finding some barcodes.\"\n",
    "            return False\n",
    "        return _name_list;\n",
    "    \n",
    "    def _check_barcode_to_gene():\n",
    "        pass\n",
    "    \n",
    "    def _check_barcode_to_region(reg_to_barcode, \n",
    "                                 pb_records=pb_records, stv_barcodes=stv_barcodes, ndb_barcodes=ndb_barcodes):\n",
    "        '''Generate map from barcode id to region id'''\n",
    "        _barcode_to_reg = {}\n",
    "        _reg_id_exists = []\n",
    "        for record in pb_records:\n",
    "            reg_id = int(record.id.split('_reg_')[1].split('_')[0]);\n",
    "            if reg_id in _reg_id_exists:\n",
    "                continue;\n",
    "            else:\n",
    "                _barcode_list = _parsing_probe_sequence(record)\n",
    "                _name_list = _finding_barcode_name(_barcode_list)\n",
    "                for _n in _name_list:\n",
    "                    if _n not in _barcode_to_reg.keys(): # create if not in dic\n",
    "                        _barcode_to_reg[_n] = [reg_id]\n",
    "                    else: # otherwise, append\n",
    "                        _barcode_to_reg[_n].append(reg_id)\n",
    "            _reg_id_exists.append(reg_id)\n",
    "        ## check region distribution\n",
    "        # invert dic from reg_to_barcode\n",
    "        _inv_dic = {}\n",
    "        for reg,bcs in sorted(reg_to_barcode.items()):\n",
    "            for bc in bcs:\n",
    "                if bc not in _inv_dic.keys():\n",
    "                    _inv_dic[bc] = [reg];\n",
    "                else:\n",
    "                    _inv_dic[bc].append(reg);\n",
    "        # compare\n",
    "        _region_check=True\n",
    "        for bc, regs in sorted(_inv_dic.items()):\n",
    "            if bc not in _barcode_to_reg.keys():\n",
    "                print \"-- \"+str(bc)+\" not in barcode_to_region dic!\"\n",
    "                _region_check = False\n",
    "                break\n",
    "            else:\n",
    "                if sorted(regs) != sorted(_barcode_to_reg[bc]):\n",
    "                    print \"-- \"+str(bc)+\" and region\"+str(regs)+\" not compatible with barcode_to_region dic!\"\n",
    "                    _region_check = False\n",
    "                    break\n",
    "                    \n",
    "        return _barcode_to_reg, _region_check\n",
    "    \n",
    "    def _check_barcode_to_color(pb_records=pb_records, stv_barcodes=stv_barcodes, ndb_barcodes=ndb_barcodes, \n",
    "                                stv_color=True, ndb_color=False,\n",
    "                                _save=save, master_dir=master_dir, save_dir=save_dir):\n",
    "        '''If multi_color is applied, generate a barcode_to_color dic for adaptor design'''\n",
    "        if 'color' not in str(pb_records[0].id):\n",
    "            print \"-- color check not applied\";\n",
    "            return False\n",
    "        elif not stv_color and not ndb_color:\n",
    "            print \"-- color check turned off in both stv and ndb\";\n",
    "            return False\n",
    "        else:\n",
    "            # get barcodes\n",
    "            _barcode_names = []\n",
    "            if stv_color: # if stv has multi-color\n",
    "                _barcode_names += [bc.id for bc in stv_barcodes];\n",
    "            if ndb_color: # if ndb has multi-color\n",
    "                _barcode_names += [bc.id for bc in ndb_barcodes];\n",
    "            # initialize color dic\n",
    "            _barcode_to_color = {};\n",
    "            _exist_regs = [];\n",
    "            # search through all probes\n",
    "            for record in pb_records:\n",
    "                _reg_id = int(record.id.split('_reg_')[1].split('_')[0]); \n",
    "                if _reg_id in _exist_regs:\n",
    "                    continue\n",
    "                else: \n",
    "                    _exist_regs.append(_reg_id);\n",
    "                _color = int(str(record.id).split('color_')[1])\n",
    "                _barcode_list = _parsing_probe_sequence(record)\n",
    "                _name_list = _finding_barcode_name(_barcode_list)\n",
    "                \n",
    "                for _name in _name_list:\n",
    "                    if _name in _barcode_names:\n",
    "                        if _name not in _barcode_to_color.keys():\n",
    "                            _barcode_to_color[_name] = [_color]\n",
    "                        else:\n",
    "                            _barcode_to_color[_name].append(_color);\n",
    "            # keep the unique colors\n",
    "            _barcode_to_unique_color = {}\n",
    "            for k,v in sorted(_barcode_to_color.items()):\n",
    "                _barcode_to_unique_color[k] = np.unique(v)\n",
    "            if _save:\n",
    "                import csv\n",
    "                # mkdir if not exist for this region\n",
    "                if not os.path.exists(master_dir+os.sep+save_dir):\n",
    "                    os.makedirs(master_dir+os.sep+save_dir)\n",
    "                with open(master_dir+os.sep+save_dir+os.sep+'color-usage.csv','w') as output_handle:\n",
    "                    fieldnames = ['barcode', 'color']\n",
    "                    writer = csv.DictWriter(output_handle, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    for _barcode, _color in sorted(_barcode_to_unique_color.items(), key=lambda (k,v):int(k.split('_')[1])):\n",
    "                        writer.writerow({'barcode': _barcode, 'color': _color})\n",
    "                \n",
    "        return _barcode_to_unique_color\n",
    "                            \n",
    "    \n",
    "    def _construct_internal_map(master_dir=master_dir, save_dir=save_dir, word_size=word_size):\n",
    "        '''Using functions in LibraryDesign, compute an internal khmer map'''\n",
    "        _int_map = khmer.Countgraph(word_size, 1e9, 2) \n",
    "        _int_map.set_use_bigcount(True)\n",
    "        _nms,_seqs = fastaread(master_dir+os.sep+save_dir+os.sep+'candidate_probes.fasta')\n",
    "        for _seq in _seqs:\n",
    "            _int_map.consume(_seq.upper())\n",
    "        return _int_map\n",
    "    \n",
    "    def _check_barcode_in_probes(barcode_to_reg, reg_size_dic, int_map, \n",
    "                                 stv_barcodes=stv_barcodes, ndb_barcodes=ndb_barcodes,\n",
    "                                 barcode_len=barcode_len, max_internal_hits=max_internal_hits):\n",
    "        '''Check barcode appearance in probes, whether that match barcode_to_region scheme'''\n",
    "        _barcode_in_probes = {}\n",
    "        for bc_name, regs in sorted(barcode_to_reg.items()):\n",
    "            bc = None\n",
    "            for _bc in stv_barcodes+ndb_barcodes:\n",
    "                if bc_name == _bc.id:\n",
    "                    bc = _bc\n",
    "                    break\n",
    "            bc_hits = int_map.get_kmer_counts( str(bc.seq[-barcode_len:].reverse_complement()).upper());\n",
    "            if max(bc_hits) - min(bc_hits) > max_internal_hits:\n",
    "                print \"-- Barcode: \"+str(bc)+\" has more off-target in different part of itself!\"\n",
    "                return False\n",
    "            else:\n",
    "                regs,reg_cts = np.unique(regs, return_counts=True);\n",
    "                bc_in_probe = 0;\n",
    "                for reg,ct in zip(regs,reg_cts):\n",
    "                    bc_in_probe += reg_size_dic[reg] * ct;\n",
    "                if max(bc_hits) - bc_in_probe > max_internal_hits:\n",
    "                    print \"-- Barcode: \"+str(bc)+\" has more off-target than threshold!\"\n",
    "                    return False\n",
    "            _barcode_in_probes[bc_name] = bc_in_probe;\n",
    "        return _barcode_in_probes, True\n",
    "    \n",
    "    def _check_between_probes(int_map, pb_lists=pb_lists, pb_records=pb_records):\n",
    "        pass \n",
    "    \n",
    "    def _check_against_genome(pb_records=pb_records, max_genome_hits=max_genome_hits, index_dir=index_dir):\n",
    "        '''Use Khmer to compare probe against genome'''\n",
    "        hg38 = khmer.load_countgraph(index_dir+os.sep+'full_word17_.kmer')\n",
    "        _failed_num = 0;\n",
    "        _keep_pb_records = [];\n",
    "        for record in pb_records:\n",
    "            _kmer_hits = hg38.get_kmer_counts(str(record.seq).upper());\n",
    "            if sum(_kmer_hits) > max_genome_hits:\n",
    "                print '-- Max_genome_hits is: '+str(max_genome_hits)+\", this seq got hits: \"+ str(sum(_kmer_hits))\n",
    "                _failed_num += 1;\n",
    "            else:\n",
    "                _keep_pb_records.append(record);\n",
    "                \n",
    "        return _keep_pb_records, _failed_num # if nothing goes wrong\n",
    "    \n",
    "    def _plot_info():\n",
    "        pass\n",
    "            \n",
    "    ## check primers\n",
    "    primer_usage = _check_primer_usage()\n",
    "    if verbose:\n",
    "        print \"\\n- 1.Passing primer usage check? -\", primer_usage\n",
    "    \n",
    "    ## check region size\n",
    "    reg_size_dic, size_match = _check_region_size()\n",
    "    if verbose:\n",
    "        print \"\\n- 2.Passing region size check? -\", size_match    \n",
    "        for k,v in sorted(reg_size_dic.items()):\n",
    "            print k,':',v\n",
    "        \n",
    "    ## check region to barcode\n",
    "    reg_to_barcode, reg2bc = _check_region_to_barcode()\n",
    "    if verbose:\n",
    "        print \"\\n- 3.Passing region to barcode mapping check? -\", reg2bc    \n",
    "        for k,v in sorted(reg_to_barcode.items(), key=lambda (k,v):k):\n",
    "            print k,':',v\n",
    "        \n",
    "    ## check barcode to region (this step must be run after step 3) \n",
    "    barcode_to_reg, bc2reg = _check_barcode_to_region(reg_to_barcode)\n",
    "    if verbose:\n",
    "        print \"\\n- 4.Passing barcode to region mapping check? -\", bc2reg    \n",
    "        for k,v in sorted(barcode_to_reg.items(), key=lambda (k,v):[k[0],int(k.split('_')[1])]):\n",
    "            print k,':',v\n",
    "    \n",
    "    ## check barcode to region (this step must be run after step 3) \n",
    "    barcode_to_color = _check_barcode_to_color()\n",
    "    if verbose:\n",
    "        print \"\\n- 5.Calculating barcode to color dictionary.\"\n",
    "        for k,v in sorted(barcode_to_color.items(), key=lambda (k,v):[k[0],int(k.split('_')[1])]):\n",
    "            print k,':',v    \n",
    "    \n",
    "    \n",
    "    ## Construct an internal map\n",
    "    int_map = _construct_internal_map();\n",
    "    if verbose:\n",
    "        print \"\\n- 6.Constructing internal khmer map\";\n",
    "    \n",
    "    ## Check barcodes total counts in probes\n",
    "    barcode_in_probes, _bc_counting = _check_barcode_in_probes(barcode_to_reg, reg_size_dic, int_map)\n",
    "    if verbose:\n",
    "        print \"\\n- 7.Passing if counting barcode appearance times in probes\", _bc_counting;    \n",
    "\n",
    "    ## Check against each other    \n",
    "    \n",
    "    ## Check against genome\n",
    "    kept_records, failed_num = _check_against_genome();\n",
    "    if verbose:\n",
    "        print \"\\n- 8.Probes not passing through genome filter:\", failed_num;  \n",
    "    \n",
    "    # check region size for kept probes\n",
    "    _size_from_rec = {}\n",
    "    for record in pb_records:\n",
    "        reg_id = int(record.id.split('_reg_')[1].split('_')[0]);\n",
    "        if reg_id not in _size_from_rec.keys():\n",
    "            _size_from_rec[reg_id] = 1; # if not in key, create\n",
    "        else:\n",
    "            _size_from_rec[reg_id] += 1; # otherwise, add count\n",
    "    if verbose:\n",
    "        print \"--  re-check region size:\"\n",
    "        for k,v in sorted(_size_from_rec.items()):\n",
    "            print k,':',v\n",
    "        print \"--- total number of probes:\", len(kept_records);\n",
    "    if save:\n",
    "        pb_savefile = master_dir + os.sep + save_dir + os.sep + 'filtered_probes.fasta';\n",
    "        if verbose:\n",
    "            print \"\\n- 9.Saving probes to:\", pb_savefile\n",
    "        with open(pb_savefile, 'w') as output_handle:\n",
    "            SeqIO.write(kept_records, output_handle, 'fasta');  \n",
    "        \n",
    "    return kept_records, _size_from_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36\n",
      "-- Checking primer usage, total probes: 28231\n",
      "\n",
      "- 1.Passing primer usage check? - True\n",
      "\n",
      "- 2.Passing region size check? - True\n",
      "0 : 215\n",
      "1 : 239\n",
      "2 : 400\n",
      "3 : 367\n",
      "4 : 400\n",
      "5 : 400\n",
      "6 : 400\n",
      "7 : 400\n",
      "8 : 400\n",
      "9 : 400\n",
      "10 : 400\n",
      "11 : 400\n",
      "12 : 400\n",
      "13 : 400\n",
      "14 : 400\n",
      "15 : 400\n",
      "16 : 400\n",
      "17 : 400\n",
      "18 : 400\n",
      "19 : 400\n",
      "20 : 400\n",
      "21 : 400\n",
      "22 : 400\n",
      "23 : 400\n",
      "24 : 400\n",
      "25 : 400\n",
      "26 : 400\n",
      "27 : 400\n",
      "28 : 400\n",
      "29 : 400\n",
      "30 : 400\n",
      "31 : 400\n",
      "32 : 385\n",
      "33 : 400\n",
      "34 : 400\n",
      "35 : 400\n",
      "36 : 400\n",
      "37 : 400\n",
      "38 : 259\n",
      "39 : 400\n",
      "40 : 400\n",
      "41 : 400\n",
      "42 : 400\n",
      "43 : 400\n",
      "44 : 400\n",
      "45 : 400\n",
      "46 : 400\n",
      "47 : 400\n",
      "48 : 400\n",
      "49 : 400\n",
      "50 : 400\n",
      "51 : 400\n",
      "52 : 400\n",
      "53 : 400\n",
      "54 : 400\n",
      "55 : 400\n",
      "56 : 400\n",
      "57 : 400\n",
      "58 : 400\n",
      "59 : 400\n",
      "60 : 400\n",
      "61 : 366\n",
      "62 : 400\n",
      "63 : 400\n",
      "64 : 400\n",
      "65 : 400\n",
      "66 : 400\n",
      "67 : 400\n",
      "68 : 400\n",
      "69 : 400\n",
      "70 : 400\n",
      "71 : 400\n",
      "\n",
      "- 3.Passing region to barcode mapping check? - True\n",
      "0 : ['Stv_3', 'Stv_4', 'NDB_37']\n",
      "1 : ['Stv_32', 'Stv_35', 'NDB_38']\n",
      "2 : ['Stv_10', 'Stv_11', 'NDB_39']\n",
      "3 : ['Stv_40', 'Stv_44', 'NDB_40']\n",
      "4 : ['Stv_42', 'Stv_44', 'NDB_41']\n",
      "5 : ['Stv_9', 'Stv_11', 'NDB_42']\n",
      "6 : ['Stv_7', 'Stv_11', 'NDB_43']\n",
      "7 : ['Stv_37', 'Stv_39', 'NDB_44']\n",
      "8 : ['Stv_7', 'Stv_10', 'NDB_45']\n",
      "9 : ['Stv_39', 'Stv_40', 'NDB_46']\n",
      "10 : ['Stv_39', 'Stv_44', 'NDB_47']\n",
      "11 : ['Stv_7', 'Stv_9', 'NDB_48']\n",
      "12 : ['Stv_9', 'Stv_10', 'NDB_49']\n",
      "13 : ['Stv_37', 'Stv_44', 'NDB_50']\n",
      "14 : ['Stv_8', 'Stv_11', 'NDB_51']\n",
      "15 : ['Stv_37', 'Stv_42', 'NDB_52']\n",
      "16 : ['Stv_36', 'Stv_42', 'NDB_53']\n",
      "17 : ['Stv_6', 'Stv_10', 'NDB_54']\n",
      "18 : ['Stv_6', 'Stv_9', 'NDB_55']\n",
      "19 : ['Stv_36', 'Stv_40', 'NDB_56']\n",
      "20 : ['Stv_6', 'Stv_11', 'NDB_57']\n",
      "21 : ['Stv_36', 'Stv_44', 'NDB_58']\n",
      "22 : ['Stv_33', 'Stv_36', 'NDB_59']\n",
      "23 : ['Stv_3', 'Stv_10', 'NDB_60']\n",
      "24 : ['Stv_8', 'Stv_10', 'NDB_61']\n",
      "25 : ['Stv_39', 'Stv_42', 'NDB_62']\n",
      "26 : ['Stv_4', 'Stv_10', 'NDB_63']\n",
      "27 : ['Stv_33', 'Stv_42', 'NDB_64']\n",
      "28 : ['Stv_35', 'Stv_42', 'NDB_65']\n",
      "29 : ['Stv_4', 'Stv_9', 'NDB_66']\n",
      "30 : ['Stv_4', 'Stv_6', 'NDB_67']\n",
      "31 : ['Stv_33', 'Stv_35', 'NDB_68']\n",
      "32 : ['Stv_5', 'Stv_10', 'NDB_69']\n",
      "33 : ['Stv_35', 'Stv_40', 'NDB_70']\n",
      "34 : ['Stv_32', 'Stv_44', 'NDB_71']\n",
      "35 : ['Stv_3', 'Stv_9', 'NDB_72']\n",
      "36 : ['Stv_4', 'Stv_8', 'NDB_73']\n",
      "37 : ['Stv_32', 'Stv_42', 'NDB_74']\n",
      "38 : ['Stv_3', 'Stv_8', 'NDB_75']\n",
      "39 : ['Stv_32', 'Stv_33', 'NDB_76']\n",
      "40 : ['Stv_37', 'Stv_40', 'NDB_77']\n",
      "41 : ['Stv_7', 'Stv_8', 'NDB_78']\n",
      "42 : ['Stv_3', 'Stv_7', 'NDB_79']\n",
      "43 : ['Stv_32', 'Stv_39', 'NDB_80']\n",
      "44 : ['Stv_5', 'Stv_9', 'NDB_81']\n",
      "45 : ['Stv_35', 'Stv_39', 'NDB_82']\n",
      "46 : ['Stv_33', 'Stv_40', 'NDB_83']\n",
      "47 : ['Stv_4', 'Stv_5', 'NDB_84']\n",
      "48 : ['Stv_6', 'Stv_7', 'NDB_85']\n",
      "49 : ['Stv_35', 'Stv_37', 'NDB_86']\n",
      "50 : ['Stv_6', 'Stv_8', 'NDB_87']\n",
      "51 : ['Stv_36', 'Stv_39', 'NDB_88']\n",
      "52 : ['Stv_40', 'Stv_42', 'NDB_89']\n",
      "53 : ['Stv_8', 'Stv_9', 'NDB_90']\n",
      "54 : ['Stv_5', 'Stv_8', 'NDB_91']\n",
      "55 : ['Stv_35', 'Stv_36', 'NDB_92']\n",
      "56 : ['Stv_5', 'Stv_7', 'NDB_93']\n",
      "57 : ['Stv_33', 'Stv_39', 'NDB_94']\n",
      "58 : ['Stv_32', 'Stv_37', 'NDB_95']\n",
      "59 : ['Stv_3', 'Stv_6', 'NDB_96']\n",
      "60 : ['Stv_5', 'Stv_11', 'NDB_97']\n",
      "61 : ['Stv_36', 'Stv_37', 'NDB_98']\n",
      "62 : ['Stv_3', 'Stv_5', 'NDB_99']\n",
      "63 : ['Stv_32', 'Stv_36', 'NDB_100']\n",
      "64 : ['Stv_33', 'Stv_44', 'NDB_101']\n",
      "65 : ['Stv_4', 'Stv_7', 'NDB_102']\n",
      "66 : ['Stv_3', 'Stv_11', 'NDB_103']\n",
      "67 : ['Stv_32', 'Stv_40', 'NDB_104']\n",
      "68 : ['Stv_5', 'Stv_6', 'NDB_105']\n",
      "69 : ['Stv_33', 'Stv_37', 'NDB_106']\n",
      "70 : ['Stv_35', 'Stv_44', 'NDB_107']\n",
      "71 : ['Stv_4', 'Stv_11', 'NDB_108']\n",
      "\n",
      "- 4.Passing barcode to region mapping check? - True\n",
      "NDB_37 : [0]\n",
      "NDB_38 : [1]\n",
      "NDB_39 : [2]\n",
      "NDB_40 : [3]\n",
      "NDB_41 : [4]\n",
      "NDB_42 : [5]\n",
      "NDB_43 : [6]\n",
      "NDB_44 : [7]\n",
      "NDB_45 : [8]\n",
      "NDB_46 : [9]\n",
      "NDB_47 : [10]\n",
      "NDB_48 : [11]\n",
      "NDB_49 : [12]\n",
      "NDB_50 : [13]\n",
      "NDB_51 : [14]\n",
      "NDB_52 : [15]\n",
      "NDB_53 : [16]\n",
      "NDB_54 : [17]\n",
      "NDB_55 : [18]\n",
      "NDB_56 : [19]\n",
      "NDB_57 : [20]\n",
      "NDB_58 : [21]\n",
      "NDB_59 : [22]\n",
      "NDB_60 : [23]\n",
      "NDB_61 : [24]\n",
      "NDB_62 : [25]\n",
      "NDB_63 : [26]\n",
      "NDB_64 : [27]\n",
      "NDB_65 : [28]\n",
      "NDB_66 : [29]\n",
      "NDB_67 : [30]\n",
      "NDB_68 : [31]\n",
      "NDB_69 : [32]\n",
      "NDB_70 : [33]\n",
      "NDB_71 : [34]\n",
      "NDB_72 : [35]\n",
      "NDB_73 : [36]\n",
      "NDB_74 : [37]\n",
      "NDB_75 : [38]\n",
      "NDB_76 : [39]\n",
      "NDB_77 : [40]\n",
      "NDB_78 : [41]\n",
      "NDB_79 : [42]\n",
      "NDB_80 : [43]\n",
      "NDB_81 : [44]\n",
      "NDB_82 : [45]\n",
      "NDB_83 : [46]\n",
      "NDB_84 : [47]\n",
      "NDB_85 : [48]\n",
      "NDB_86 : [49]\n",
      "NDB_87 : [50]\n",
      "NDB_88 : [51]\n",
      "NDB_89 : [52]\n",
      "NDB_90 : [53]\n",
      "NDB_91 : [54]\n",
      "NDB_92 : [55]\n",
      "NDB_93 : [56]\n",
      "NDB_94 : [57]\n",
      "NDB_95 : [58]\n",
      "NDB_96 : [59]\n",
      "NDB_97 : [60]\n",
      "NDB_98 : [61]\n",
      "NDB_99 : [62]\n",
      "NDB_100 : [63]\n",
      "NDB_101 : [64]\n",
      "NDB_102 : [65]\n",
      "NDB_103 : [66]\n",
      "NDB_104 : [67]\n",
      "NDB_105 : [68]\n",
      "NDB_106 : [69]\n",
      "NDB_107 : [70]\n",
      "NDB_108 : [71]\n",
      "Stv_3 : [0, 23, 35, 38, 42, 59, 62, 66]\n",
      "Stv_4 : [0, 26, 29, 30, 36, 47, 65, 71]\n",
      "Stv_5 : [32, 44, 47, 54, 56, 60, 62, 68]\n",
      "Stv_6 : [17, 18, 20, 30, 48, 50, 59, 68]\n",
      "Stv_7 : [6, 8, 11, 41, 42, 48, 56, 65]\n",
      "Stv_8 : [14, 24, 36, 38, 41, 50, 53, 54]\n",
      "Stv_9 : [5, 11, 12, 18, 29, 35, 44, 53]\n",
      "Stv_10 : [2, 8, 12, 17, 23, 24, 26, 32]\n",
      "Stv_11 : [2, 5, 6, 14, 20, 60, 66, 71]\n",
      "Stv_32 : [1, 34, 37, 39, 43, 58, 63, 67]\n",
      "Stv_33 : [22, 27, 31, 39, 46, 57, 64, 69]\n",
      "Stv_35 : [1, 28, 31, 33, 45, 49, 55, 70]\n",
      "Stv_36 : [16, 19, 21, 22, 51, 55, 61, 63]\n",
      "Stv_37 : [7, 13, 15, 40, 49, 58, 61, 69]\n",
      "Stv_39 : [7, 9, 10, 25, 43, 45, 51, 57]\n",
      "Stv_40 : [3, 9, 19, 33, 40, 46, 52, 67]\n",
      "Stv_42 : [4, 15, 16, 25, 27, 28, 37, 52]\n",
      "Stv_44 : [3, 4, 10, 13, 21, 34, 64, 70]\n",
      "\n",
      "- 5.Calculating barcode to color dictionary.\n",
      "Stv_3 : [0]\n",
      "Stv_4 : [0]\n",
      "Stv_5 : [0]\n",
      "Stv_6 : [0]\n",
      "Stv_7 : [0]\n",
      "Stv_8 : [0]\n",
      "Stv_9 : [0]\n",
      "Stv_10 : [0]\n",
      "Stv_11 : [0]\n",
      "Stv_32 : [1]\n",
      "Stv_33 : [1]\n",
      "Stv_35 : [1]\n",
      "Stv_36 : [1]\n",
      "Stv_37 : [1]\n",
      "Stv_39 : [1]\n",
      "Stv_40 : [1]\n",
      "Stv_42 : [1]\n",
      "Stv_44 : [1]\n",
      "\n",
      "- 6.Constructing internal khmer map\n",
      "\n",
      "- 7.Passing if counting barcode appearance times in probes True\n",
      "-- Max_genome_hits is: 150, this seq got hits: 477\n",
      "-- Max_genome_hits is: 150, this seq got hits: 501\n",
      "-- Max_genome_hits is: 150, this seq got hits: 203\n",
      "-- Max_genome_hits is: 150, this seq got hits: 444\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1057\n",
      "-- Max_genome_hits is: 150, this seq got hits: 323\n",
      "-- Max_genome_hits is: 150, this seq got hits: 413\n",
      "-- Max_genome_hits is: 150, this seq got hits: 235\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 162\n",
      "-- Max_genome_hits is: 150, this seq got hits: 571\n",
      "-- Max_genome_hits is: 150, this seq got hits: 229\n",
      "-- Max_genome_hits is: 150, this seq got hits: 244\n",
      "-- Max_genome_hits is: 150, this seq got hits: 225\n",
      "-- Max_genome_hits is: 150, this seq got hits: 188\n",
      "-- Max_genome_hits is: 150, this seq got hits: 368\n",
      "-- Max_genome_hits is: 150, this seq got hits: 170\n",
      "-- Max_genome_hits is: 150, this seq got hits: 178\n",
      "-- Max_genome_hits is: 150, this seq got hits: 466\n",
      "-- Max_genome_hits is: 150, this seq got hits: 233\n",
      "-- Max_genome_hits is: 150, this seq got hits: 224\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 162\n",
      "-- Max_genome_hits is: 150, this seq got hits: 186\n",
      "-- Max_genome_hits is: 150, this seq got hits: 3474\n",
      "-- Max_genome_hits is: 150, this seq got hits: 186\n",
      "-- Max_genome_hits is: 150, this seq got hits: 413\n",
      "-- Max_genome_hits is: 150, this seq got hits: 243\n",
      "-- Max_genome_hits is: 150, this seq got hits: 212\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1229\n",
      "-- Max_genome_hits is: 150, this seq got hits: 271\n",
      "-- Max_genome_hits is: 150, this seq got hits: 414\n",
      "-- Max_genome_hits is: 150, this seq got hits: 390\n",
      "-- Max_genome_hits is: 150, this seq got hits: 228\n",
      "-- Max_genome_hits is: 150, this seq got hits: 153\n",
      "-- Max_genome_hits is: 150, this seq got hits: 183\n",
      "-- Max_genome_hits is: 150, this seq got hits: 167\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 194\n",
      "-- Max_genome_hits is: 150, this seq got hits: 177\n",
      "-- Max_genome_hits is: 150, this seq got hits: 181\n",
      "-- Max_genome_hits is: 150, this seq got hits: 460\n",
      "-- Max_genome_hits is: 150, this seq got hits: 179\n",
      "-- Max_genome_hits is: 150, this seq got hits: 155\n",
      "-- Max_genome_hits is: 150, this seq got hits: 274\n",
      "-- Max_genome_hits is: 150, this seq got hits: 217\n",
      "-- Max_genome_hits is: 150, this seq got hits: 242\n",
      "-- Max_genome_hits is: 150, this seq got hits: 185\n",
      "-- Max_genome_hits is: 150, this seq got hits: 190\n",
      "-- Max_genome_hits is: 150, this seq got hits: 224\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1022\n",
      "-- Max_genome_hits is: 150, this seq got hits: 214\n",
      "-- Max_genome_hits is: 150, this seq got hits: 300\n",
      "-- Max_genome_hits is: 150, this seq got hits: 303\n",
      "-- Max_genome_hits is: 150, this seq got hits: 162\n",
      "-- Max_genome_hits is: 150, this seq got hits: 234\n",
      "-- Max_genome_hits is: 150, this seq got hits: 833\n",
      "-- Max_genome_hits is: 150, this seq got hits: 229\n",
      "-- Max_genome_hits is: 150, this seq got hits: 304\n",
      "-- Max_genome_hits is: 150, this seq got hits: 160\n",
      "-- Max_genome_hits is: 150, this seq got hits: 594\n",
      "-- Max_genome_hits is: 150, this seq got hits: 175\n",
      "-- Max_genome_hits is: 150, this seq got hits: 186\n",
      "-- Max_genome_hits is: 150, this seq got hits: 242\n",
      "-- Max_genome_hits is: 150, this seq got hits: 411\n",
      "-- Max_genome_hits is: 150, this seq got hits: 563\n",
      "-- Max_genome_hits is: 150, this seq got hits: 211\n",
      "-- Max_genome_hits is: 150, this seq got hits: 637\n",
      "-- Max_genome_hits is: 150, this seq got hits: 390\n",
      "-- Max_genome_hits is: 150, this seq got hits: 357\n",
      "-- Max_genome_hits is: 150, this seq got hits: 175\n",
      "-- Max_genome_hits is: 150, this seq got hits: 224\n",
      "-- Max_genome_hits is: 150, this seq got hits: 252\n",
      "-- Max_genome_hits is: 150, this seq got hits: 955\n",
      "-- Max_genome_hits is: 150, this seq got hits: 164\n",
      "-- Max_genome_hits is: 150, this seq got hits: 169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Max_genome_hits is: 150, this seq got hits: 170\n",
      "-- Max_genome_hits is: 150, this seq got hits: 180\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1198\n",
      "-- Max_genome_hits is: 150, this seq got hits: 161\n",
      "-- Max_genome_hits is: 150, this seq got hits: 166\n",
      "-- Max_genome_hits is: 150, this seq got hits: 206\n",
      "-- Max_genome_hits is: 150, this seq got hits: 205\n",
      "-- Max_genome_hits is: 150, this seq got hits: 350\n",
      "-- Max_genome_hits is: 150, this seq got hits: 267\n",
      "-- Max_genome_hits is: 150, this seq got hits: 9080\n",
      "-- Max_genome_hits is: 150, this seq got hits: 206\n",
      "-- Max_genome_hits is: 150, this seq got hits: 365\n",
      "-- Max_genome_hits is: 150, this seq got hits: 238\n",
      "-- Max_genome_hits is: 150, this seq got hits: 199\n",
      "-- Max_genome_hits is: 150, this seq got hits: 287\n",
      "-- Max_genome_hits is: 150, this seq got hits: 197\n",
      "-- Max_genome_hits is: 150, this seq got hits: 241\n",
      "-- Max_genome_hits is: 150, this seq got hits: 255\n",
      "-- Max_genome_hits is: 150, this seq got hits: 516\n",
      "-- Max_genome_hits is: 150, this seq got hits: 152\n",
      "-- Max_genome_hits is: 150, this seq got hits: 940\n",
      "-- Max_genome_hits is: 150, this seq got hits: 267\n",
      "-- Max_genome_hits is: 150, this seq got hits: 223\n",
      "-- Max_genome_hits is: 150, this seq got hits: 549\n",
      "-- Max_genome_hits is: 150, this seq got hits: 333\n",
      "-- Max_genome_hits is: 150, this seq got hits: 203\n",
      "-- Max_genome_hits is: 150, this seq got hits: 220\n",
      "-- Max_genome_hits is: 150, this seq got hits: 4027\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1213\n",
      "-- Max_genome_hits is: 150, this seq got hits: 249\n",
      "-- Max_genome_hits is: 150, this seq got hits: 172\n",
      "-- Max_genome_hits is: 150, this seq got hits: 320\n",
      "-- Max_genome_hits is: 150, this seq got hits: 418\n",
      "-- Max_genome_hits is: 150, this seq got hits: 158\n",
      "-- Max_genome_hits is: 150, this seq got hits: 191\n",
      "-- Max_genome_hits is: 150, this seq got hits: 241\n",
      "-- Max_genome_hits is: 150, this seq got hits: 236\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 162\n",
      "-- Max_genome_hits is: 150, this seq got hits: 191\n",
      "-- Max_genome_hits is: 150, this seq got hits: 5027\n",
      "-- Max_genome_hits is: 150, this seq got hits: 157\n",
      "-- Max_genome_hits is: 150, this seq got hits: 682\n",
      "-- Max_genome_hits is: 150, this seq got hits: 221\n",
      "-- Max_genome_hits is: 150, this seq got hits: 157\n",
      "-- Max_genome_hits is: 150, this seq got hits: 202\n",
      "-- Max_genome_hits is: 150, this seq got hits: 351\n",
      "-- Max_genome_hits is: 150, this seq got hits: 372\n",
      "-- Max_genome_hits is: 150, this seq got hits: 6719\n",
      "-- Max_genome_hits is: 150, this seq got hits: 210\n",
      "-- Max_genome_hits is: 150, this seq got hits: 193\n",
      "-- Max_genome_hits is: 150, this seq got hits: 183\n",
      "-- Max_genome_hits is: 150, this seq got hits: 158\n",
      "-- Max_genome_hits is: 150, this seq got hits: 159\n",
      "-- Max_genome_hits is: 150, this seq got hits: 475\n",
      "-- Max_genome_hits is: 150, this seq got hits: 164\n",
      "-- Max_genome_hits is: 150, this seq got hits: 207\n",
      "-- Max_genome_hits is: 150, this seq got hits: 444\n",
      "-- Max_genome_hits is: 150, this seq got hits: 153\n",
      "-- Max_genome_hits is: 150, this seq got hits: 289\n",
      "-- Max_genome_hits is: 150, this seq got hits: 433\n",
      "-- Max_genome_hits is: 150, this seq got hits: 218\n",
      "-- Max_genome_hits is: 150, this seq got hits: 242\n",
      "-- Max_genome_hits is: 150, this seq got hits: 285\n",
      "-- Max_genome_hits is: 150, this seq got hits: 222\n",
      "-- Max_genome_hits is: 150, this seq got hits: 347\n",
      "-- Max_genome_hits is: 150, this seq got hits: 215\n",
      "-- Max_genome_hits is: 150, this seq got hits: 196\n",
      "-- Max_genome_hits is: 150, this seq got hits: 157\n",
      "-- Max_genome_hits is: 150, this seq got hits: 188\n",
      "-- Max_genome_hits is: 150, this seq got hits: 158\n",
      "-- Max_genome_hits is: 150, this seq got hits: 5781\n",
      "-- Max_genome_hits is: 150, this seq got hits: 169\n",
      "-- Max_genome_hits is: 150, this seq got hits: 188\n",
      "-- Max_genome_hits is: 150, this seq got hits: 163\n",
      "-- Max_genome_hits is: 150, this seq got hits: 179\n",
      "-- Max_genome_hits is: 150, this seq got hits: 2027\n",
      "-- Max_genome_hits is: 150, this seq got hits: 358\n",
      "-- Max_genome_hits is: 150, this seq got hits: 175\n",
      "-- Max_genome_hits is: 150, this seq got hits: 174\n",
      "-- Max_genome_hits is: 150, this seq got hits: 6708\n",
      "-- Max_genome_hits is: 150, this seq got hits: 175\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1702\n",
      "-- Max_genome_hits is: 150, this seq got hits: 159\n",
      "-- Max_genome_hits is: 150, this seq got hits: 151\n",
      "-- Max_genome_hits is: 150, this seq got hits: 192\n",
      "-- Max_genome_hits is: 150, this seq got hits: 190\n",
      "-- Max_genome_hits is: 150, this seq got hits: 183\n",
      "-- Max_genome_hits is: 150, this seq got hits: 187\n",
      "-- Max_genome_hits is: 150, this seq got hits: 305\n",
      "-- Max_genome_hits is: 150, this seq got hits: 275\n",
      "-- Max_genome_hits is: 150, this seq got hits: 164\n",
      "-- Max_genome_hits is: 150, this seq got hits: 192\n",
      "-- Max_genome_hits is: 150, this seq got hits: 440\n",
      "-- Max_genome_hits is: 150, this seq got hits: 223\n",
      "-- Max_genome_hits is: 150, this seq got hits: 868\n",
      "-- Max_genome_hits is: 150, this seq got hits: 186\n",
      "-- Max_genome_hits is: 150, this seq got hits: 178\n",
      "-- Max_genome_hits is: 150, this seq got hits: 159\n",
      "-- Max_genome_hits is: 150, this seq got hits: 195\n",
      "-- Max_genome_hits is: 150, this seq got hits: 161\n",
      "-- Max_genome_hits is: 150, this seq got hits: 190\n",
      "-- Max_genome_hits is: 150, this seq got hits: 169\n",
      "-- Max_genome_hits is: 150, this seq got hits: 197\n",
      "-- Max_genome_hits is: 150, this seq got hits: 167\n",
      "-- Max_genome_hits is: 150, this seq got hits: 157\n",
      "-- Max_genome_hits is: 150, this seq got hits: 211\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1882\n",
      "-- Max_genome_hits is: 150, this seq got hits: 242\n",
      "-- Max_genome_hits is: 150, this seq got hits: 234\n",
      "-- Max_genome_hits is: 150, this seq got hits: 2273\n",
      "-- Max_genome_hits is: 150, this seq got hits: 516\n",
      "-- Max_genome_hits is: 150, this seq got hits: 155\n",
      "-- Max_genome_hits is: 150, this seq got hits: 153\n",
      "-- Max_genome_hits is: 150, this seq got hits: 196\n",
      "-- Max_genome_hits is: 150, this seq got hits: 243\n",
      "-- Max_genome_hits is: 150, this seq got hits: 448\n",
      "-- Max_genome_hits is: 150, this seq got hits: 376\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1383\n",
      "-- Max_genome_hits is: 150, this seq got hits: 257\n",
      "-- Max_genome_hits is: 150, this seq got hits: 341\n",
      "-- Max_genome_hits is: 150, this seq got hits: 204\n",
      "-- Max_genome_hits is: 150, this seq got hits: 401\n",
      "-- Max_genome_hits is: 150, this seq got hits: 170\n",
      "-- Max_genome_hits is: 150, this seq got hits: 413\n",
      "-- Max_genome_hits is: 150, this seq got hits: 160\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1929\n",
      "-- Max_genome_hits is: 150, this seq got hits: 250\n",
      "-- Max_genome_hits is: 150, this seq got hits: 218\n",
      "-- Max_genome_hits is: 150, this seq got hits: 333\n",
      "-- Max_genome_hits is: 150, this seq got hits: 597\n",
      "-- Max_genome_hits is: 150, this seq got hits: 178\n",
      "-- Max_genome_hits is: 150, this seq got hits: 165\n",
      "-- Max_genome_hits is: 150, this seq got hits: 195\n",
      "-- Max_genome_hits is: 150, this seq got hits: 178\n",
      "-- Max_genome_hits is: 150, this seq got hits: 173\n",
      "-- Max_genome_hits is: 150, this seq got hits: 222\n",
      "-- Max_genome_hits is: 150, this seq got hits: 215\n",
      "-- Max_genome_hits is: 150, this seq got hits: 169\n",
      "-- Max_genome_hits is: 150, this seq got hits: 249\n",
      "-- Max_genome_hits is: 150, this seq got hits: 221\n",
      "-- Max_genome_hits is: 150, this seq got hits: 252\n",
      "-- Max_genome_hits is: 150, this seq got hits: 389\n",
      "-- Max_genome_hits is: 150, this seq got hits: 306\n",
      "-- Max_genome_hits is: 150, this seq got hits: 580\n",
      "-- Max_genome_hits is: 150, this seq got hits: 171\n",
      "-- Max_genome_hits is: 150, this seq got hits: 452\n",
      "-- Max_genome_hits is: 150, this seq got hits: 250\n",
      "-- Max_genome_hits is: 150, this seq got hits: 314\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 303\n",
      "-- Max_genome_hits is: 150, this seq got hits: 200\n",
      "-- Max_genome_hits is: 150, this seq got hits: 157\n",
      "-- Max_genome_hits is: 150, this seq got hits: 174\n",
      "-- Max_genome_hits is: 150, this seq got hits: 448\n",
      "-- Max_genome_hits is: 150, this seq got hits: 178\n",
      "-- Max_genome_hits is: 150, this seq got hits: 856\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1669\n",
      "-- Max_genome_hits is: 150, this seq got hits: 203\n",
      "-- Max_genome_hits is: 150, this seq got hits: 208\n",
      "-- Max_genome_hits is: 150, this seq got hits: 278\n",
      "-- Max_genome_hits is: 150, this seq got hits: 215\n",
      "-- Max_genome_hits is: 150, this seq got hits: 266\n",
      "-- Max_genome_hits is: 150, this seq got hits: 539\n",
      "-- Max_genome_hits is: 150, this seq got hits: 184\n",
      "-- Max_genome_hits is: 150, this seq got hits: 179\n",
      "-- Max_genome_hits is: 150, this seq got hits: 460\n",
      "-- Max_genome_hits is: 150, this seq got hits: 530\n",
      "-- Max_genome_hits is: 150, this seq got hits: 210\n",
      "-- Max_genome_hits is: 150, this seq got hits: 178\n",
      "-- Max_genome_hits is: 150, this seq got hits: 270\n",
      "-- Max_genome_hits is: 150, this seq got hits: 219\n",
      "-- Max_genome_hits is: 150, this seq got hits: 153\n",
      "-- Max_genome_hits is: 150, this seq got hits: 177\n",
      "-- Max_genome_hits is: 150, this seq got hits: 186\n",
      "-- Max_genome_hits is: 150, this seq got hits: 346\n",
      "-- Max_genome_hits is: 150, this seq got hits: 175\n",
      "-- Max_genome_hits is: 150, this seq got hits: 553\n",
      "-- Max_genome_hits is: 150, this seq got hits: 407\n",
      "-- Max_genome_hits is: 150, this seq got hits: 402\n",
      "-- Max_genome_hits is: 150, this seq got hits: 173\n",
      "-- Max_genome_hits is: 150, this seq got hits: 152\n",
      "-- Max_genome_hits is: 150, this seq got hits: 361\n",
      "-- Max_genome_hits is: 150, this seq got hits: 937\n",
      "-- Max_genome_hits is: 150, this seq got hits: 193\n",
      "-- Max_genome_hits is: 150, this seq got hits: 288\n",
      "-- Max_genome_hits is: 150, this seq got hits: 241\n",
      "-- Max_genome_hits is: 150, this seq got hits: 356\n",
      "-- Max_genome_hits is: 150, this seq got hits: 158\n",
      "-- Max_genome_hits is: 150, this seq got hits: 178\n",
      "-- Max_genome_hits is: 150, this seq got hits: 199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Max_genome_hits is: 150, this seq got hits: 221\n",
      "-- Max_genome_hits is: 150, this seq got hits: 162\n",
      "-- Max_genome_hits is: 150, this seq got hits: 217\n",
      "-- Max_genome_hits is: 150, this seq got hits: 162\n",
      "-- Max_genome_hits is: 150, this seq got hits: 230\n",
      "-- Max_genome_hits is: 150, this seq got hits: 322\n",
      "-- Max_genome_hits is: 150, this seq got hits: 163\n",
      "-- Max_genome_hits is: 150, this seq got hits: 253\n",
      "-- Max_genome_hits is: 150, this seq got hits: 207\n",
      "-- Max_genome_hits is: 150, this seq got hits: 272\n",
      "-- Max_genome_hits is: 150, this seq got hits: 168\n",
      "-- Max_genome_hits is: 150, this seq got hits: 560\n",
      "-- Max_genome_hits is: 150, this seq got hits: 191\n",
      "-- Max_genome_hits is: 150, this seq got hits: 519\n",
      "-- Max_genome_hits is: 150, this seq got hits: 273\n",
      "-- Max_genome_hits is: 150, this seq got hits: 561\n",
      "-- Max_genome_hits is: 150, this seq got hits: 220\n",
      "-- Max_genome_hits is: 150, this seq got hits: 175\n",
      "-- Max_genome_hits is: 150, this seq got hits: 170\n",
      "-- Max_genome_hits is: 150, this seq got hits: 172\n",
      "-- Max_genome_hits is: 150, this seq got hits: 180\n",
      "-- Max_genome_hits is: 150, this seq got hits: 151\n",
      "-- Max_genome_hits is: 150, this seq got hits: 165\n",
      "-- Max_genome_hits is: 150, this seq got hits: 337\n",
      "-- Max_genome_hits is: 150, this seq got hits: 152\n",
      "-- Max_genome_hits is: 150, this seq got hits: 295\n",
      "-- Max_genome_hits is: 150, this seq got hits: 184\n",
      "-- Max_genome_hits is: 150, this seq got hits: 160\n",
      "-- Max_genome_hits is: 150, this seq got hits: 156\n",
      "-- Max_genome_hits is: 150, this seq got hits: 509\n",
      "-- Max_genome_hits is: 150, this seq got hits: 161\n",
      "-- Max_genome_hits is: 150, this seq got hits: 179\n",
      "-- Max_genome_hits is: 150, this seq got hits: 220\n",
      "-- Max_genome_hits is: 150, this seq got hits: 191\n",
      "-- Max_genome_hits is: 150, this seq got hits: 308\n",
      "-- Max_genome_hits is: 150, this seq got hits: 156\n",
      "-- Max_genome_hits is: 150, this seq got hits: 244\n",
      "-- Max_genome_hits is: 150, this seq got hits: 351\n",
      "-- Max_genome_hits is: 150, this seq got hits: 444\n",
      "-- Max_genome_hits is: 150, this seq got hits: 227\n",
      "-- Max_genome_hits is: 150, this seq got hits: 624\n",
      "-- Max_genome_hits is: 150, this seq got hits: 259\n",
      "-- Max_genome_hits is: 150, this seq got hits: 165\n",
      "-- Max_genome_hits is: 150, this seq got hits: 249\n",
      "-- Max_genome_hits is: 150, this seq got hits: 191\n",
      "-- Max_genome_hits is: 150, this seq got hits: 197\n",
      "-- Max_genome_hits is: 150, this seq got hits: 173\n",
      "-- Max_genome_hits is: 150, this seq got hits: 159\n",
      "-- Max_genome_hits is: 150, this seq got hits: 336\n",
      "-- Max_genome_hits is: 150, this seq got hits: 163\n",
      "-- Max_genome_hits is: 150, this seq got hits: 282\n",
      "-- Max_genome_hits is: 150, this seq got hits: 166\n",
      "-- Max_genome_hits is: 150, this seq got hits: 295\n",
      "-- Max_genome_hits is: 150, this seq got hits: 328\n",
      "-- Max_genome_hits is: 150, this seq got hits: 220\n",
      "-- Max_genome_hits is: 150, this seq got hits: 157\n",
      "-- Max_genome_hits is: 150, this seq got hits: 848\n",
      "-- Max_genome_hits is: 150, this seq got hits: 702\n",
      "-- Max_genome_hits is: 150, this seq got hits: 289\n",
      "-- Max_genome_hits is: 150, this seq got hits: 169\n",
      "-- Max_genome_hits is: 150, this seq got hits: 181\n",
      "-- Max_genome_hits is: 150, this seq got hits: 367\n",
      "-- Max_genome_hits is: 150, this seq got hits: 264\n",
      "-- Max_genome_hits is: 150, this seq got hits: 274\n",
      "-- Max_genome_hits is: 150, this seq got hits: 170\n",
      "-- Max_genome_hits is: 150, this seq got hits: 269\n",
      "-- Max_genome_hits is: 150, this seq got hits: 200\n",
      "-- Max_genome_hits is: 150, this seq got hits: 237\n",
      "-- Max_genome_hits is: 150, this seq got hits: 270\n",
      "-- Max_genome_hits is: 150, this seq got hits: 334\n",
      "-- Max_genome_hits is: 150, this seq got hits: 159\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 156\n",
      "-- Max_genome_hits is: 150, this seq got hits: 158\n",
      "-- Max_genome_hits is: 150, this seq got hits: 292\n",
      "-- Max_genome_hits is: 150, this seq got hits: 230\n",
      "-- Max_genome_hits is: 150, this seq got hits: 171\n",
      "-- Max_genome_hits is: 150, this seq got hits: 195\n",
      "-- Max_genome_hits is: 150, this seq got hits: 171\n",
      "-- Max_genome_hits is: 150, this seq got hits: 430\n",
      "-- Max_genome_hits is: 150, this seq got hits: 391\n",
      "-- Max_genome_hits is: 150, this seq got hits: 217\n",
      "-- Max_genome_hits is: 150, this seq got hits: 162\n",
      "-- Max_genome_hits is: 150, this seq got hits: 245\n",
      "-- Max_genome_hits is: 150, this seq got hits: 161\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 215\n",
      "-- Max_genome_hits is: 150, this seq got hits: 276\n",
      "-- Max_genome_hits is: 150, this seq got hits: 155\n",
      "-- Max_genome_hits is: 150, this seq got hits: 204\n",
      "-- Max_genome_hits is: 150, this seq got hits: 328\n",
      "-- Max_genome_hits is: 150, this seq got hits: 174\n",
      "-- Max_genome_hits is: 150, this seq got hits: 292\n",
      "-- Max_genome_hits is: 150, this seq got hits: 292\n",
      "-- Max_genome_hits is: 150, this seq got hits: 212\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 310\n",
      "-- Max_genome_hits is: 150, this seq got hits: 305\n",
      "-- Max_genome_hits is: 150, this seq got hits: 200\n",
      "-- Max_genome_hits is: 150, this seq got hits: 411\n",
      "-- Max_genome_hits is: 150, this seq got hits: 171\n",
      "-- Max_genome_hits is: 150, this seq got hits: 199\n",
      "-- Max_genome_hits is: 150, this seq got hits: 446\n",
      "-- Max_genome_hits is: 150, this seq got hits: 407\n",
      "-- Max_genome_hits is: 150, this seq got hits: 159\n",
      "-- Max_genome_hits is: 150, this seq got hits: 308\n",
      "-- Max_genome_hits is: 150, this seq got hits: 229\n",
      "-- Max_genome_hits is: 150, this seq got hits: 192\n",
      "-- Max_genome_hits is: 150, this seq got hits: 276\n",
      "-- Max_genome_hits is: 150, this seq got hits: 379\n",
      "-- Max_genome_hits is: 150, this seq got hits: 300\n",
      "-- Max_genome_hits is: 150, this seq got hits: 182\n",
      "-- Max_genome_hits is: 150, this seq got hits: 170\n",
      "-- Max_genome_hits is: 150, this seq got hits: 163\n",
      "-- Max_genome_hits is: 150, this seq got hits: 564\n",
      "-- Max_genome_hits is: 150, this seq got hits: 234\n",
      "-- Max_genome_hits is: 150, this seq got hits: 249\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1609\n",
      "-- Max_genome_hits is: 150, this seq got hits: 318\n",
      "-- Max_genome_hits is: 150, this seq got hits: 155\n",
      "-- Max_genome_hits is: 150, this seq got hits: 378\n",
      "-- Max_genome_hits is: 150, this seq got hits: 196\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1316\n",
      "-- Max_genome_hits is: 150, this seq got hits: 167\n",
      "-- Max_genome_hits is: 150, this seq got hits: 182\n",
      "-- Max_genome_hits is: 150, this seq got hits: 195\n",
      "-- Max_genome_hits is: 150, this seq got hits: 153\n",
      "-- Max_genome_hits is: 150, this seq got hits: 251\n",
      "-- Max_genome_hits is: 150, this seq got hits: 162\n",
      "-- Max_genome_hits is: 150, this seq got hits: 329\n",
      "-- Max_genome_hits is: 150, this seq got hits: 257\n",
      "-- Max_genome_hits is: 150, this seq got hits: 188\n",
      "-- Max_genome_hits is: 150, this seq got hits: 402\n",
      "-- Max_genome_hits is: 150, this seq got hits: 152\n",
      "-- Max_genome_hits is: 150, this seq got hits: 260\n",
      "-- Max_genome_hits is: 150, this seq got hits: 203\n",
      "-- Max_genome_hits is: 150, this seq got hits: 186\n",
      "-- Max_genome_hits is: 150, this seq got hits: 360\n",
      "-- Max_genome_hits is: 150, this seq got hits: 256\n",
      "-- Max_genome_hits is: 150, this seq got hits: 349\n",
      "-- Max_genome_hits is: 150, this seq got hits: 166\n",
      "-- Max_genome_hits is: 150, this seq got hits: 154\n",
      "-- Max_genome_hits is: 150, this seq got hits: 164\n",
      "-- Max_genome_hits is: 150, this seq got hits: 496\n",
      "-- Max_genome_hits is: 150, this seq got hits: 342\n",
      "-- Max_genome_hits is: 150, this seq got hits: 173\n",
      "-- Max_genome_hits is: 150, this seq got hits: 196\n",
      "-- Max_genome_hits is: 150, this seq got hits: 371\n",
      "-- Max_genome_hits is: 150, this seq got hits: 230\n",
      "-- Max_genome_hits is: 150, this seq got hits: 316\n",
      "-- Max_genome_hits is: 150, this seq got hits: 528\n",
      "-- Max_genome_hits is: 150, this seq got hits: 152\n",
      "-- Max_genome_hits is: 150, this seq got hits: 206\n",
      "-- Max_genome_hits is: 150, this seq got hits: 388\n",
      "-- Max_genome_hits is: 150, this seq got hits: 312\n",
      "-- Max_genome_hits is: 150, this seq got hits: 385\n",
      "-- Max_genome_hits is: 150, this seq got hits: 256\n",
      "-- Max_genome_hits is: 150, this seq got hits: 402\n",
      "-- Max_genome_hits is: 150, this seq got hits: 319\n",
      "-- Max_genome_hits is: 150, this seq got hits: 328\n",
      "-- Max_genome_hits is: 150, this seq got hits: 474\n",
      "-- Max_genome_hits is: 150, this seq got hits: 314\n",
      "-- Max_genome_hits is: 150, this seq got hits: 333\n",
      "-- Max_genome_hits is: 150, this seq got hits: 187\n",
      "-- Max_genome_hits is: 150, this seq got hits: 241\n",
      "-- Max_genome_hits is: 150, this seq got hits: 153\n",
      "-- Max_genome_hits is: 150, this seq got hits: 319\n",
      "-- Max_genome_hits is: 150, this seq got hits: 152\n",
      "-- Max_genome_hits is: 150, this seq got hits: 481\n",
      "-- Max_genome_hits is: 150, this seq got hits: 224\n",
      "-- Max_genome_hits is: 150, this seq got hits: 463\n",
      "-- Max_genome_hits is: 150, this seq got hits: 428\n",
      "-- Max_genome_hits is: 150, this seq got hits: 207\n",
      "-- Max_genome_hits is: 150, this seq got hits: 1323\n",
      "-- Max_genome_hits is: 150, this seq got hits: 205\n",
      "-- Max_genome_hits is: 150, this seq got hits: 171\n",
      "-- Max_genome_hits is: 150, this seq got hits: 178\n",
      "-- Max_genome_hits is: 150, this seq got hits: 320\n",
      "-- Max_genome_hits is: 150, this seq got hits: 192\n",
      "-- Max_genome_hits is: 150, this seq got hits: 187\n",
      "-- Max_genome_hits is: 150, this seq got hits: 164\n",
      "-- Max_genome_hits is: 150, this seq got hits: 204\n",
      "-- Max_genome_hits is: 150, this seq got hits: 359\n",
      "-- Max_genome_hits is: 150, this seq got hits: 174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- 8.Probes not passing through genome filter: 451\n",
      "--  re-check region size:\n",
      "0 : 215\n",
      "1 : 239\n",
      "2 : 400\n",
      "3 : 367\n",
      "4 : 400\n",
      "5 : 400\n",
      "6 : 400\n",
      "7 : 400\n",
      "8 : 400\n",
      "9 : 400\n",
      "10 : 400\n",
      "11 : 400\n",
      "12 : 400\n",
      "13 : 400\n",
      "14 : 400\n",
      "15 : 400\n",
      "16 : 400\n",
      "17 : 400\n",
      "18 : 400\n",
      "19 : 400\n",
      "20 : 400\n",
      "21 : 400\n",
      "22 : 400\n",
      "23 : 400\n",
      "24 : 400\n",
      "25 : 400\n",
      "26 : 400\n",
      "27 : 400\n",
      "28 : 400\n",
      "29 : 400\n",
      "30 : 400\n",
      "31 : 400\n",
      "32 : 385\n",
      "33 : 400\n",
      "34 : 400\n",
      "35 : 400\n",
      "36 : 400\n",
      "37 : 400\n",
      "38 : 259\n",
      "39 : 400\n",
      "40 : 400\n",
      "41 : 400\n",
      "42 : 400\n",
      "43 : 400\n",
      "44 : 400\n",
      "45 : 400\n",
      "46 : 400\n",
      "47 : 400\n",
      "48 : 400\n",
      "49 : 400\n",
      "50 : 400\n",
      "51 : 400\n",
      "52 : 400\n",
      "53 : 400\n",
      "54 : 400\n",
      "55 : 400\n",
      "56 : 400\n",
      "57 : 400\n",
      "58 : 400\n",
      "59 : 400\n",
      "60 : 400\n",
      "61 : 366\n",
      "62 : 400\n",
      "63 : 400\n",
      "64 : 400\n",
      "65 : 400\n",
      "66 : 400\n",
      "67 : 400\n",
      "68 : 400\n",
      "69 : 400\n",
      "70 : 400\n",
      "71 : 400\n",
      "--- total number of probes: 27780\n",
      "\n",
      "- 9.Saving probes to: /n/boslfs/LABS/zhuang_lab/User/pzheng/Libraries/CTP-03/chr21_9by36/final_probes/filtered_probes.fasta\n"
     ]
    }
   ],
   "source": [
    "print master_dir;\n",
    "\n",
    "kept_records, kept_size_dic = Check_Probes(pb_records, pb_lists, sub_encodings, master_dir, \n",
    "                                           total_bc=3, save_dir = pb_dir,\n",
    "                                           fwd_primer=fprimer, rev_primer=rprimer,\n",
    "                                           stv_barcodes=stv_barcodes, ndb_barcodes=ndb_barcodes,\n",
    "                                           max_genome_hits=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
