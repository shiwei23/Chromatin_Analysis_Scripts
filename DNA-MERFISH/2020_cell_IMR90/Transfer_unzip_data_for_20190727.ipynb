{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae3a07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import blosc\n",
    "import time\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return f.read()\n",
    "\n",
    "def write_compressed(filepath, bytedata):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(bytedata)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # choose the compression and file extension you want here\n",
    "    # zstandard gives good compression rates and is fast for clevel = 1\n",
    "    compression_type = 'zstd'\n",
    "    compression_level = 1 # fast for zstandard\n",
    "    file_ext = '.zst' # file extension to append to dax file\n",
    "    \n",
    "    # this will delete the dax file once the compressed file is written\n",
    "    delete = True\n",
    "    \n",
    "    # check the data by reading it back in uncompressing, and checking against the original\n",
    "    # unecessary...\n",
    "    check = False\n",
    "    \n",
    "    \n",
    "    for root, dirs, files in os.walk(os.getcwd()):\n",
    "        for file in files:\n",
    "            if file.endswith('.dax'):\n",
    "                \n",
    "                file_path = os.path.join(root, file)\n",
    "                file_size = os.stat(file_path).st_size\n",
    "                \n",
    "                print('working on: ' + str(file_path))\n",
    "                \n",
    "                t0 = time.time()\n",
    "                raw = read_data(file_path)\n",
    "                \n",
    "                t1 = time.time()\n",
    "                compressed = blosc.compress(raw, cname = compression_type, clevel = compression_level)\n",
    "                t2 = time.time()\n",
    "                \n",
    "                compressed_path = file_path + file_ext\n",
    "                write_compressed(compressed_path, compressed)\n",
    "                compressed_size = os.stat(compressed_path).st_size\n",
    "                print('compression time: ' + '{:.2f}'.format(t2-t1))\n",
    "                print('total time: ' + '{:.2f}'.format(t2-t0))\n",
    "                \n",
    "                # delete the uncompressed file after it is compressed \n",
    "                if delete:\n",
    "                    deleted = False\n",
    "                    if os.path.exists(compressed_path):\n",
    "                        os.remove(file_path)\n",
    "                        deleted = True\n",
    "                        \n",
    "                # read back in the data to see if it is the same\n",
    "                if check:\n",
    "                    decompressed = blosc.decompress(read_data(compressed_path))\n",
    "                    sameQ = raw == decompressed\n",
    "\n",
    "\n",
    "                logfile_path = os.path.join(root, 'compression_log.txt')\n",
    "                \n",
    "                # write something to a log\n",
    "                with open(logfile_path, 'a+') as logfile:\n",
    "                    logfile.write('compressing file: ' + file_path + '\\n')\n",
    "                    logfile.write('compression time: ' + '{:.2f}'.format(t2 - t1) + ' s\\n' )\n",
    "                    logfile.write('total time: ' + '{:.2f}'.format(t2 - t0) + ' s\\n' )\n",
    "                    logfile.write('compression amount: ' + str(int(compressed_size/1024)) + '/' + str(int(file_size/1024)) + '\\t' + '{:.2f}'.format(100 * compressed_size/file_size) + '%\\n')\n",
    "                    \n",
    "                    if check:\n",
    "                        if  sameQ:\n",
    "                            logfile.write('compression passed test\\n')\n",
    "                        else:\n",
    "                            logfile.write('compression failed test\\n')\n",
    "                    if not delete:\n",
    "                        logfile.write('dax file not removed\\n')\n",
    "                    else:\n",
    "                        if deleted:\n",
    "                            logfile.write('dax file removed\\n')\n",
    "                        else:\n",
    "                            logfile.write('warning check files\\n')\n",
    "                    logfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fae1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_folder = r'\\\\10.245.74.218\\Raw_data\\Bogdan\\7_27_2019_IMR90RNA'\n",
    "tar_folder = r'\\\\10.245.74.212\\Chromatin_NAS_2\\20190727_IMR90_intron-DNA-MERFISH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3bfdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fds = [os.path.join(src_folder, _fl) for _fl in os.listdir(src_folder) if _fl[0]=='H' or _fl[:2]=='Ab']\n",
    "\n",
    "fov_basenames = [_fl.split(os.extsep)[0] for _fl in sorted(os.listdir(full_fds[0])) if '.dax' in _fl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e909fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "import blosc\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f9323f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2905\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sel_fov_ids = np.arange(5,10)\n",
    "\n",
    "source_filenames, target_filenames = [], []\n",
    "\n",
    "for _fov_id in sel_fov_ids:\n",
    "    _fov_basename = fov_basenames[_fov_id]\n",
    "    for _fd in full_fds:\n",
    "        # target_fls \n",
    "        _src_fls = [os.path.join(_fd, _fl) for _fl in os.listdir(_fd) if _fov_basename in _fl]\n",
    "        _tar_fls = [_fl.replace(src_folder, tar_folder) for _fl in _src_fls]\n",
    "        \n",
    "        # append\n",
    "        source_filenames.extend(_src_fls)\n",
    "        target_filenames.extend(_tar_fls)\n",
    "print(len(source_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce809f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2657it [6:51:18,  9.27s/it]"
     ]
    }
   ],
   "source": [
    "overwrite = False\n",
    "verbose = False\n",
    "\n",
    "for _src_fl, _tar_fl in tqdm(zip(source_filenames, target_filenames)):\n",
    "    \n",
    "    _tar_fd = os.path.dirname(_tar_fl)\n",
    "    if not os.path.exists(_tar_fd):\n",
    "        if verbose:\n",
    "            print(f\"Create folder: {_tar_fd}\")\n",
    "        os.makedirs(_tar_fd)\n",
    "    \n",
    "    if _src_fl.split(os.extsep)[-1] == 'zst':\n",
    "        _tar_fl = _tar_fl.split('.zst')[0]\n",
    "        if overwrite or not os.path.exists(_tar_fl):\n",
    "            if verbose:\n",
    "                print(f\"decopress from {_src_fl} to {_tar_fl}\")\n",
    "            decompressed = blosc.decompress(read_data(_src_fl))\n",
    "            write_compressed(_tar_fl, decompressed)\n",
    "    else:\n",
    "        if overwrite or not os.path.exists(_tar_fl):\n",
    "            if verbose:\n",
    "                print(f\"copy from {_src_fl} to {_tar_fl}\")\n",
    "            copyfile(_src_fl, _tar_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638ab44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
